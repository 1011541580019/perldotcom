<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" id="sixapart-standard">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="generator" content="Movable Type Pro 5.02" />
<link rel="stylesheet" href="/pub/styles.css" type="text/css" />
<link rel="start" href="/pub/" title="Home" />
<link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/pub/atom.xml" />
<script type="text/javascript" src="/pub/mt.js"></script>

<script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-50555-22']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type =
   'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
   'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0];
   s.parentNode.insertBefore(ga, s);
 })();

</script>
    <title>Perl.com: Databases Archives</title>


</head>
<body id="perl-com" class="mt-archive-listing mt-category-archive layout-wt">
    <div id="container">
        <div id="container-inner">


            <div id="header">
    <div id="header-inner">
        <div id="header-content">
        <div id="top_advert"> 
<!-- Put any landscape advert in here -->
<a href="http://www.perlfoundation.org/" target="_new">
<img src="/i/tpf_banner.png" width="468" height="60" /></a>
        </div> 



            <div id="header-name"><a href="/pub/" accesskey="1">Perl.com</a></div>
            <div id="header-description"></div>




        </div>
    </div>
</div>



            <div id="content">
                <div id="content-inner">


                    <div id="alpha">
                        <div id="alpha-inner">

                            
                            <h1 id="page-title" class="archive-title">Recently in <em>Databases</em> Category</h1>






                            
                            <div id="entry-800" class="entry-asset asset hentry">
    <div class="asset-header">
        <h2 class="asset-name entry-title"><a href="/pub/2005/07/test-mockdbi.html" rel="bookmark">An Introduction to Test::MockDBI</a></h2>
        <div class="asset-meta">
            <span class="byline">
    
                By <span class="vcard author">Mark Leighton Fisher</span> on <abbr class="published" title="2005-07-21T00:00:00-08:00">July 21, 2005 12:00 AM</abbr>
    
            </span>

            
            

        </div>
    </div>
    <div class="asset-content entry-content">

        <div class="asset-body">
            
<!-- sidebar begins -->
<!-- don't move sidebars-->
<!-- sidebar ends -->
<h3>Prelude</h3>

<p>How do you test DBI programs:</p>

<ul>

<li>Without having to modify your current program code or environment
settings?</li>

<li>Without having to set up multiple test databases?</li>

<li>Without separating your test data from your test code?</li>

<li>With tests for every bizarre value your program will ever have to
face?</li>

<li>With complete control over all database return values, along with all DBI
method return values?</li>

<li>With an easy, regex-based rules interface?</li>

</ul>

<p>You test with <a
href="http://search.cpan.org/perldoc?Test::MockDBI">Test::MockDBI</a>, that's
how. Test::MockDBI provides all of this by using Test::MockObject::Extends to
mock up the entire DBI API.  Without a solution like Test::MockDBI--a
solution that enables direct manipulation of the DBI--you'll have to trace
DBI methods through a series of test databases.</p>

<p>You can make test databases work, but:</p>

<ul>

<li>You'll need multiple (perhaps many) databases when you need multiple sets
of mutually inconsistent values for complete test coverage.</li>

<li>Some DBI failure modes are impossible to generate through any test
database.</li>

<li>Depending on the database toolset available, it may be difficult to insert
all necessary test values--for example, Unicode values in ASCII applications,
or bizarre file types in a document-manager application.</li>

<li>Test databases, by definition, are separate from their corresponding test
code. This increases the chance that the test code and the test data will fall
out of sync with each other.</li>

</ul>

<p>Using Test::MockDBI avoids these problems. Read on to learn how
Test::MockDBI eases the job of testing DBI applications.</p>

<!-- sidebar begins -->
 <csperl file="grab" domain="on" record="b/1795" template="b/article_sidebar.view">
<!-- sidebar ends -->

<h3>A Mock Up of the Entire DBI</h3>

<p>Test::MockDBI mocks up the entire DBI API by using <a
href="http://search.cpan.org/perldoc?Test::MockObject::Extends">Test::MockObject::Extends</a>
to substitute a Test::MockObject::Extends object in place of the <a
href="http://search.cpan.org/perldoc?DBI">DBI</a>. A feature of this approach
is that if the DBI API changes (and you use that change), you will notice
during testing if you haven't upgraded Test::MockDBI, as your program will
complain about missing DBI API method(s).</p>

<p>Mocking up the entire DBI means that you can add the DBI testing code into
an existing application without changing the initial application code--using
Test::MockDBI is entirely transparent to the rest of your application, as it
neither knows nor cares that it's using Test::MockDBI in place of the DBI.
This property of transparency is what drove me to develop Test::MockDBI, as it
meant I could add the Test::MockDBI DBI testing code to existing client
applications without modifying the existing code (handy, for us
consultants).</p>

<p>Further enhancing Test::MockDBI's transparency is the <code>DBI testing type</code>
class value. Testing is only enabled when the DBI testing type is non-zero, so
you can just leave the DBI testing code additions in your production code--users will not even know about your DBI testing code unless you tell them.</p>

<p>Mocking up the entire DBI also means that you have complete control of the
DBI's behavior during testing. Often, you can simulate a <code>SELECT</code>
DBI transaction with a simple state machine that returns just a few rows from
the (mocked up) database. Test::MockDBI lets you use a <code>CODEREF</code> to supply
database return values, so you can easily put a simple state machine into the
<code>CODEREF</code> to supply the necessary database values for testing. You could even put
a delay loop into the <code>CODEREF</code> when you need to perform speed tests on your
code.</p>

<h3>Rules-Based DBI Testing</h3>

<p>You control the mocked-up DBI of Test::MockDBI with one or more rules that
you insert as Test::MockDBI method calls into your program. The default DBI
method values provided by Test::MockDBI make the database appear to have a hole
in the bottom of it--all method calls return OK, but you can't get any data
out of the database. Rules for DBI methods that return database values (the
<code>fetch*()</code> and <code>select*()</code> methods) can use either a
value that they return directly for matching method calls, or a <code>CODEREF</code> called
to provide a value each time that rule fires. A rule matches when its DBI
testing type is the current testing type and the current SQL matches the rule's
regular expression. Rules fire in the order in which you declare them, so
usually you want to order your rules from most-specific to least-specific.</p>

<p>The DBI testing type is an unsigned integer matching <code>/^d+$/</code>.
When the DBI testing type is zero, there will be no DBI testing (or at least, no
mocked-up DBI testing) performed, and the program will use the DBI normally. A
zero DBI testing type value in a rule means the rule could fire for any
non-zero DBI testing type value--that is, zero is the wildcard DBI testing
type value for rules. Set the DBI testing type either by a first command-line
argument of the form:</p>

<pre><code>--dbitest[=DTT]</code></pre>

<p>where the optional <code>DTT</code> is the DBI testing type (defaulting to
one), or through Test::MockDBI's <code>set_dbi_test_type()</code> method.
Setting the DBI testing type through a first command-line argument has the
advantage of requiring no modifications to the code under test, as this
command-line processing is done so early (during <code>BEGIN</code> time for
Test::MockDBI) that the code under test should be ignorant of whether this
processing ever happened.</p>













<h3>DBI Return Values</h3>

<p>Test::MockDBI defaults to returning a success (true) value for all DBI
method calls. This fits well with the usual techniques of DBI programming,
where the first DBI error causes the program to stop what it is doing.
Test::MockDBI's <code>bad_method()</code> method creates a rule that forces a
failure return value on the specified DBI method when the current DBI testing
type and SQL match those of the rule. Arbitrary DBI method return value
failures like these are difficult (at best) to generate with a test database.</p>

<p>Test::MockDBI's <code>set_retval_scalar()</code> and
<code>set_retval_array()</code> methods create rules for what database values
to return. Set rules for scalar return values (<code>arrayrefs</code> and <code>hashrefs</code>) with
<code>set_retval_scalar()</code> and for array return value rules with
<code>set_retval_array()</code>. You can supply a value to be returned every
time the rule matches, which is good when extracting single rows out of the
database, such as configuration parameters.  Alternatively, pass a <code>CODEREF</code> that
will be called each time the rule fires to return a new value.  Commonly, with
<code>SELECT</code> statements, the DBI returns one or more rows, then returns an empty row
to signify the end of the data. A <code>CODEREF</code> can incorporate a state machine that
implements this &quot;return 1+ rows, then a terminator&quot; behavior quite easily.
Having individual state machines for each rule is much easier to develop with than
having one master state machine embedded into Test::MockDBI's core. (An early
alpha of Test::MockDBI used the master state machine approach, so I have
empirical evidence of this result--I am not emptily theorizing here.)</p>

<p>Depending on what tools you have for creating your test databases, it may be
difficult to populate the test database with all of the values you need to test
against. Although it is probably not so much the case today, only a few years
ago populating a database with Unicode was difficult, given the
national-charset-based tools of the day. Even today, a document management
system might be difficult to populate with weird file types. Test::MockDBI
makes these kinds of tests much easier to carry out, as you directly specify
the data for the mock database to return rather than using a separate test
database.</p>

<p>This ease of database value testing also applies when you need to test
against combinations of database values that are unlikely to occur in practice
(the old &quot;comparing apples to battleships&quot; problem). If you need to handle
database value corruption--as in network problems causing the return of
partial values from a Chinese database when the program is in the U.S.--this ability to completely specify the database return values could be
invaluable in testing. Test::MockDBI lets you take complete control of your
database return values without separating test code and test data.</p>

<h3>Simplicity: Test::MockDBI's Standard-Output-Based Interface</h3>

<p>This modern incarnation of the age-old stubbed-functions technique also uses
the old technique of &quot;<code>printf()</code> and scratch head&quot; as its output
interface. This being Perl we are working with, and not FORTRAN IV (thank
goodness), we have multiple options beyond the use of unvarnished standard
output.</p>

<p>One option that I think integrates well with DBI-using module testing is to
redirect standard output into a string using <a
href="http://search.cpan.org/perldoc?IO::String">IO::String</a>. You can then
match the string against the regex you are looking for. As you have already
guessed, use of pure standard output integrates well with command-line program
testing.</p>

<p>What you will look for, irrespective of where your code actually looks, is
the output of each DBI method as it executes--the method name and arguments--along with anything else your code writes to standard output.</p>

<h3>Bind Test Data to Test Code</h3>

<p>Because DBI and database return values are bound to your test programs when
using Test::MockDBI, there is less risk of test data getting out of sync with
the test code. A separate test database introduces another point of failure in
your testing process. Multiple test databases add yet another point of failure
for each database. Whatever you use to generate the test databases also
introduces another point of failure for each database.  I can imagine cases
where special-purpose programs for generating test databases might create
multiple points of failure, especially if the programs have to integrate data
from multiple sources to generate the test data (such as a VMS Bill of Materials
database and a Solaris PCB CAD file for a test database generation program
running on Linux).</p>

<p>One of the major advances in software engineering is the increasing ability
to gather and control related information together--the 1990s advance of
object-oriented programming in common languages is a testimony to this, from
which we Perl programmers reap the benefits in our use of CPAN. For many
testing purposes, there is no need for separate test databases. Without that
need for a separate test database, separating test data from test code only
complicates the testing process.  Test::MockDBI lets you bind together your
test code and test data into one nice, neat package. Binding is even closer
than code and comments, as comments can get out of sync with their code, while
the test code and test data for Test::MockDBI cannot get out of sync too far
without causing their tests to fail unexpectedly.</p>

<h3>When to Use Test::MockDBI</h3>

<p>DBI's <code>trace()</code>, <a
href="http://search.cpan.org/perldoc?DBD::Mock">DBD::Mock</a>, and
Test::MockDBI are complementary solutions to the problem of testing DBI
software. DBI's <code>trace()</code> is a pure tracing mechanism, as it does
not change the data returned from the database or the DBI method return values.
DBD::Mock works at level of a database driver, so you have to look at your DBI
testing from the driver's point of view, rather than the DBI caller's point of
view. DBD::Mock also requires that your code supports configurable DBI DSNs,
which may not be the case in all circumstances, especially when you must
maintain or enhance legacy DBI software.</p>

<p>Test::MockDBI works at the DBI caller's level, which is (IMHO) more natural
for testing DBI-using software (possibly a matter of taste: TMTOWTDI).
Test::MockDBI's interface with your DBI software is a set of easy-to-program,
regex-based rules, which incorporate a lot of power into one or a few lines of
code, thereby using Perl's built-in regex support to best advantage. This
binds test data and test code tightly together, reducing the chance of
synchronization problems between the test data and the test code. Using
Test::MockDBI does not require modifying the current code of the DBI software
being tested, as you only need additional code to enable Test::MockDBI-driven
DBI testing.</p>

<p>Test::MockDBI takes additional coding effort when you need to test DBI
program performance.  It may be that for performance testing, you want to use
test databases rather than Test::MockDBI. If you were in any danger of your
copy of <em>DBI.pm</em> becoming corrupted, I don't know whether you could
adequately test that condition with Test::MockDBI, depending on the corruption.
You would probably have to create a special mock DBI to test corrupted DBI code
handling, though you could start building the special mock DBI by inheriting
from Test::MockDBI without any problems from Test::MockDBI's design, as it
should be inheritance-friendly.</p>













<h3>Some Examples</h3>

<p>To make:</p>

<pre><code>$dbh = DBI-&gt;connect(&quot;dbi:AZ:universe&quot;, &quot;mortal&quot;, &quot;(none)&quot;);</code></pre>

<p>fail, add the rule:</p>

<pre><code>$tmd-&gt;bad_method(&quot;connect&quot;, 1,
    &quot;CONNECT TO dbi:AZ:universe AS mortal WITH \\(none\\)&quot;);</code></pre>

<p>(where <code>$tmd</code> is the only Test::MockDBI object, which you obtain
through Test::MockDBI's <code>get_instance()</code> method).</p>

<p>To make a SQL <code>SELECT</code> failure when using
<code>DBI::execute()</code>, use the rule:</p>

<pre><code>$tmd-&gt;bad_method(&quot;execute&quot;, 1,
    &quot;SELECT zip_plus_4 from zipcodes where state='IN'&quot;);</code></pre>

<p>This rule implies that:</p>

<ul>

<li>The <code>DBI::connect()</code> <code>succeeded()</code>.</li>

<li>The <code>DBI::prepare()</code> <code>succeeded()</code>.</li>

<li>But the <code>DBI::execute()</code> failed as it should.</li>

</ul>

<p>A common use of direct scalar return values is returning configuration data,
such as a U.S. zip code for an address:</p>

<pre><code>$tmd-&gt;set_retval_scalar(1,
 &quot;zip5.*'IN'.*'NOBLESVILLE'.*'170 WESTFIELD RD'&quot;,
 [ 46062 ]);</code></pre>

<p>This demonstrates using a regular expression, as matching SQL could then
look like this:</p>

<pre><code>SELECT
  zip5
FROM
  zipcodes
WHERE
  state='IN' AND
  city='NOBLESVILLE' AND
  street_address='170 WESTFIELD RD'</code></pre>

<p>and the rule would match.</p>

<p><code>SELECT</code>s that return one or more rows from the database are the
common case:</p>

<pre><code>my $counter = 0;                    # name counter
sub possibly_evil_names {
    $counter++;
    if ($counter == 1) {
        return ('Adolf', 'Germany');
    } elsif ($counter == 2) {
        return ('Josef', 'U.S.S.R.');
    } else {
        return ();
    }
}
$tmd-&gt;set_retval_array(1,
   &quot;SELECT\\s+name,\\s+country.*possibly_evil_names&quot;,
   \&amp;possibly_evil_names);</code></pre>

<p>Using a <code>CODEREF</code> (<code>\&amp;possibly_evil_names</code>) lets you easily add
the state machine for implementing a return of two names followed by an empty
array (because the code uses <code>fetchrow_array()</code> to retrieve each
row). SQL for this query could look like:</p>

<pre><code>SELECT
  name,
  country
FROM
  possibly_evil_names
WHERE
  year &lt; 2000</code></pre>

<h3>Summary</h3>

<p>Albert Einstein once said, &quot;Everything should be made as simple as possible,
but no simpler.&quot; This is what I have striven for while developing Test::MockDBI--the simplest possible useful module for testing DBI programs by mocking up
the entire DBI.</p>

<p>Test::MockDBI gives you:</p>

<ul>

<li>Complete control of DBI return values and database-returned data.</li>

<li>Returned database values from either direct value specifications or
<code>CODEREF</code>-generated values.</li>

<li>Easy, regex-based rules that govern the DBI's behavior, along with
intelligent defaults for the common cases.</li>

<li>Complete transparency to other code, so the code under test neither knows
nor cares that you are testing it with Test::MockDBI.</li>

<li>Test data tightly bound to test code, which promotes cohesiveness in your
testing environment, thereby reducing the chance that your tests might silently
fail due to loss of synchronization between your test data and your test
code.</li>

</ul>

<p>Test::MockDBI is a valuable addition to the arsenal of DBI testing
techniques.</p>





        </div>



    </div>
    <div class="asset-footer"></div>
</div>


                            
                            <div id="entry-768" class="entry-asset asset hentry">
    <div class="asset-header">
        <h2 class="asset-name entry-title"><a href="/pub/2005/05/aggregation.html" rel="bookmark">Massive Data Aggregation with Perl</a></h2>
        <div class="asset-meta">
            <span class="byline">
    
                By <span class="vcard author">Fred Moyer</span> on <abbr class="published" title="2005-05-05T00:00:00-08:00">May  5, 2005 12:00 AM</abbr>
    
            </span>

            
            

        </div>
    </div>
    <div class="asset-content entry-content">

        <div class="asset-body">
            
<!-- sidebar begins -->
<!-- don't move sidebars-->
<!-- sidebar ends -->
<p>This article is a case study of the use of <a
href="http://www.perl.org/">Perl</a> and <a
href="http://www.w3.org/XML/">XML</a>/<a href="http://www.w3.org/RDF/">RDF</a>
technologies to channel disparate sources of data into a semi-structured
repository. This repository helped to build structured <a
href="http://en.wikipedia.org/wiki/OLAP">OLAP</a> warehouses by mining an RDF
repository with <a
href="http://search.cpan.org/~rbs/XML-SAX-Machines-0.41/lib/XML/SAX/Machines.pm">SAX
machines</a>. Channels of data included user-contributed datasets, data from
FTP and HTTP remote-based repositories, and data from other intra-enterprise
based assets.  We called the system the 'Kitchen Sync', but one of the project's visionaries best described it as akin to a device that accepts piles of random
coins and returns them sorted for analysis. This system collected voter data
and was the primary data collection point in a national organization for the
presidential campaign during the 2004 election.</p>

<h3>Introduction</h3>

<p>My initial question was why anyone would want to store data in XML/RDF
formats. It's verbose, it lacks widely accepted query interfaces (such as SQL),
and it generally requires more work than a database. XML, in particular, is a
great messaging interface, but a poor persistence medium.</p>

<p>Eventually, I concluded that this particular implementation did benefit from
the use of XML and RDF as messaging protocols. The messaging interface involved
the use of SAX machines to parse a queue of XML and RDF files. The XML files
contained the metadata for what we called polls, and the RDF files contained
data from those polls. We had a very large buffer, from which cron-based
processes frequently constructed data warehouses for analysis.</p>

<csperl file="grab" domain="on" record="b/708" template="b/article_sidebar.view">

<h3>Hindsight and Realizations</h3>

<p>The difficulty of this project was in the gathering of requirements and
vendor interfacing. When implementing application workflow, it is critical to
use a programming language that doesn't get in the way and allows you to do
what you want--and that is where Perl really shined. A language that allows
for quick development is an asset, especially in a rushed environment where
projects are due &quot;yesterday&quot;. The code samples here are not examples of how to
write great object-oriented Perl code. They are real world examples of the code
used to get things done in this project.</p>

<p>For example, when a voter-data vendor changed its poll format, our data
collection spiders stopped returned data and alerted our staff immediately. In
just minutes, we adapted our SAX machine to the vendor's new format and we had
our data streams back up and running. It would have taken hours or days to call
the vendor about the change and engage in a technical discussion to get them to
do things our way. Instead, Perl allowed us to adapt to their ways quickly and
efficiently.</p>

<h3>Project Goals</h3>

<p>The architects of this project specified several goals and metrics for the
application. The main goals--with the penultimate objective being to
accumulate as much data as possible before election day--were to:</p>

<ul>

<li><p>Develop a web-based application for defining metadata of polls, and
uploading sets of poll data to the system.</p>

<p>The application had to give the user the ability to define sets of questions
and answers known as polls. Poll metadata could contain related data contained
in documents of standard business formats (<em>.doc</em>, <em>.pdf</em>).
The users also needed an easy method, one that minimized possible errors,
to upload data to the system.</p></li>

<li><p>Meet requirements of adding 50 million new records per day.</p>

<p>That metric corresponds to approximately 578 records per second.
Assuming a non-linear load distribution over time, peak transaction
requirements were likely to be orders of magnitude higher than the
average of 578 per second.</p></li>

<li><p>Develop a persistent store for RDF and XML data representing
polls and poll data.</p>

<p>The web application had to generate XML documents from poll definitions and
RDF documents from uploaded poll data.  We stored the poll data in RDF. We
needed an API to manage these documents.</p></li>

<li><p>Develop a mechanized data collection system for the retrieval of data from
FTP- and HTTP-based data repositories.</p>

<p>The plan was to assimilate data sources into our organization from several
commercial and other types of vendors. Most vendors had varying schemas and
formats for their data. We wanted to acquire as much data as possible before
the election to gauge voter support levels and other key metrics crucial to
winning a political election.</p></li>

</ul>

<h3>Web Application</h3>

<p>When I started this project, I had been using <a
href="http://perl.apache.org/docs/2.0/index.html">mod_perl2</a> extensively in
prototyping applications and also as a means of finding all of the cool new
features. Mod_perl2 had proven itself stable enough to use in production, so I
implemented a Model-View-Controller application design pattern using a native
mod_perl2 and an <a
href="http://httpd.apache.org/apreq/docs/libapreq2/">libapreq2</a>-enabled <a
href="http://httpd.apache.org/">Apache server</a>. I adopted the controller
design patterns from recipes in the <a
href="http://www.modperlcookbook.org/">Modperl Cookbook</a>. The model classes
subclassed <a href="http://www.sleepycat.com/products/xml.shtml">Berkeley
DBXML</a> and <a
href="http://search.cpan.org/perldoc?XML::LibXML">XML::LibXML</a> for object
methods and persistence.  We used <a
href="http://search.cpan.org/perldoc?Template">Template Toolkit</a> to
implement views. (I will present more about the specifics of the persistence
layer later in this article.)</p>

<p>Of primary importance with the web application component of the system was
ease of use. If the system was not easy to use, then we would likely receive
less data as a result of user frustration. The component of the web application
that took extended transaction processing time was the poll data upload
component.</p>

<p>If the user uploads a 10MB file on a 10Kbps upstream
connection (common for residential DSL lines), the transaction would take
approximately twenty minutes. On a 100Kbps upstream connection
(business grade DSL), the transaction would take two minutes--certainly much
longer than most unsuspecting users would wait before clicking on the browser
refresh button.</p>

<p>To prevent the user from accidentally corrupting the lengthy upload process,
I created a monitoring browser window which opened via the following <a
href="http://www.mozilla.org/js/">JavaScript</a> call when the user clicked the
upload button.</p>

<pre><code>&lt;input type=submit name='submit' value='Upload'
    onClick=&quot;window.open('/ksync/dataset/monitor', 'Upload',
       'width=740,height=400')&quot;&gt;</code></pre>

<p>The server forked off a child process which read the upload status from a <a
href="http://www.sleepycat.com/products/db.shtml">BerkeleyDB</a> database. The
parent process used a <a href="http://httpd.apache.org/apreq/">libapreq</a> <a
href="http://httpd.apache.org/apreq/docs/libapreq2/group__apreq__xs__request.html#item_upload_hook">UPLOAD_HOOK</a>-based approach to measure the amount of data uploaded, and to write that plus a few other metrics to the BerkeleyDB database. The following is a snippet of
code from the upload handler:</p>

<pre><code>&lt;Location /ksync/poll/data/progress&gt;
    PerlResponseHandler KSYNC::Apache::Data::Upload-&gt;progress
&lt;/Location&gt;

sub progress : method {
    my ( $self, $r ) = @_;

    # We deal with commas and tabs as delimiters currently
    my $delimiter;

    # Create a BerkeleyDB to keep track of upload progress
    my $db = _init_status_db( DB_CREATE );

    # Get the specifics of the poll we're getting data for
    my $poll = $r-&gt;pnotes('SESSION')-&gt;{'poll'};

    # Generate a unique identifier for files based on the poll
    my $id = _file_id($poll);

    # Store any data which does not validate according to the poll schema
    my $invalid = IO::File-&gt;new();
    my $ivfn = join '', $config-&gt;get('data_root'), '/invalid/', $id, '.txt';
    $invalid-&gt;open(&quot;&gt; $ivfn&quot;);

    # Set the rdf filename
    my $gfn = join '', $config-&gt;get('data_root'), '/valid/', $id, '.rdf';

    # Create an RDF document object to store the data
    my $rdf = KSYNC::Model::Poll::Data::RDF-&gt;new(
                $gfn, 
                $poll,
                $r-&gt;pnotes('SESSION')-&gt;{'creator'}, 
                DateTime-&gt;now-&gt;ymd, 
    );

    # Get the poll questions for to make sure the answers are valid
    my $questions = $poll-&gt;questions;

    # Create a data structure to hold the answers to validate against.
    my @valid_answers = _valid_answers($questions);

    # And a data structure to hold the validation results
    my $question_data = KSYNC::Model::Poll::validation_results($questions);

    # Set progress store parameters
    my $length              = 0;
    my $good_lines_total    = 0;
    my $invalid_lines_total = 0;
    my $began;              # Boolean to determine if we've started parsing data
    my $li                  = 1;    # Starting line number

    # The subroutine to process uploaded data
    my $fragment;
    my $upload_hook = sub {
        my ( $upload, $data, $data_len, $hook_data ) = @_;

        if ( !$began ) {   # If this is the first set check the array length

            # Chop up the stream
            my @lines = split &quot;\n&quot;, $data;

            # Determine the delimiter for this line
            $delimiter = _delimiter(@lines);

            unless ( ( split( /$delimiter/, $lines[0] ) ) ==
                scalar( @{$question_data} ) + 1 )
            {
                $db-&gt;db_put( 'done', '1' );
                
                # The dataset isn't valid, so throw an exception
                KSYNC::Apache::Exception-&gt;throw('Invalid Dataset!');
            }
        }

        # Mark the start up the upload
        $began = 1;

        # Validate the data against the poll answers we've defined
        my ( $good_lines, $invalid_lines );

        ( $good_lines, $invalid_lines, $question_data, $li, $fragment ) =
          KSYNC::Model::Poll::Data::validate( \@valid_answers, 
                                              $data, 
                                              $question_data,
                                              $li, 
                                              $delimiter, 
                                              $fragment );

        # Keep up the running count of good and invalid lines
        $good_lines_total     += scalar( @{$good_lines} );
        $invalid_lines_total  += scalar( @{$invalid_lines} );

        # Increment the number of bytes processed
        $length += length($data);

        # Update the status for the monitor process
        $db-&gt;db_put(
                     valid     =&gt; $good_lines_total,
                     invalid   =&gt; $invalid_lines_total,
                     bytes     =&gt; $length,
                     filename  =&gt; $upload-&gt;filename,
                     filetype  =&gt; $upload-&gt;type,
                     questions =&gt; $question_data,
                   );

        # And store the data we've collected
        $rdf-&gt;write( $good_lines ) if scalar( @{$good_lines} );

        # Write out any invalid data points to a separate file
        _write_txt( $invalid, $invalid_lines ) if scalar( @{$invalid_lines} );
    };

    my $req = Apache::Request-&gt;new(
        $r,
        POST_MAX    =&gt; 1024 * 1024 * 1024,    # One Gigabyte
        HOOK_DATA   =&gt; 'Note',
        UPLOAD_HOOK =&gt; $upload_hook,
        TEMP_DIR    =&gt; $config-&gt;get('temp_dir'),
    );

    my $upload = eval { $req-&gt;upload( scalar +( $req-&gt;upload )[0] ) };
    if ( ref $@ and $@-&gt;isa(&quot;Apache::Request::Error&quot;) ) {

        # ... handle Apache::Request::Error object in $@
        $r-&gt;headers_out-&gt;set( Location =&gt; 'https://'
              . $r-&gt;construct_server
              . '/ksync/poll/data/upload/aborted' );
        return Apache::REDIRECT;
    }

    # Finish up
    $invalid-&gt;close;
    $rdf-&gt;save;

    # Set status so the progress window will close
    $db-&gt;db_put('done', 1');
    undef $db;
    
    # Send the user to the summary page
    $r-&gt;headers_out-&gt;set(
      Location =&gt; join('', 
                       'https://', 
                       $r-&gt;construct_server, 
                       '/poll/data/upload/summary',
                      )                   
    );
    return Apache::REDIRECT; 
}</code></pre>

<p>During the upload process, the users saw a status window which refreshed
every two seconds and had a pleasant animated GIF to enhance their experience,
as well as several metrics on the status of the upload. One user uploaded a
file that took 45 minutes because of a degraded network connection, but the
uploaded file had no errors.</p>

<p>The system converted <a
href="http://en.wikipedia.org/wiki/Comma-separated_values">CSV</a> files that
users uploaded into RDF and saved them to the RDF store during the upload
process. Because of the use of the UPLOAD_HOOK approach for processing uploaded
data, the mod_perl-enabled Apache processes never grew in size or leaked memory
as a result of handling the upload content.</p>













<h3>Poll and Poll Data Stores</h3>

<p>Several parties involved raised questions about the use of XML and RDF as
persistence mediums. Why not use a relational database? Our primary reasons for
deciding against a relational database were that we had several different
schemas and formats of incoming data, and we needed to be able to absorb huge
influxes of data in very short time periods.</p>

<p>Consider how a relational database could have handled the variation in
schemas and formats. Creating vendor-specific drivers to handle each format
would have been straightforward. To handle the variations in schema, we could
have normalized each data stream and its attributes so that we could store all
the data in source, object, attribute, and value tables. The problem with that
approach is that you get one really big table with all the values, which
becomes more difficult to manage as time goes on. Another possible approach,
which I have used in the past, is to create separate tables for each data
stream to fit the schema, and then use the power of left, right, and outer
joins to extract the needed information. It scales much better than the first
approach but it is not as well suited for data mining as warehouses are.</p>

<p>With regard to absorbing a lot of data very quickly, transactional
relational databases have limitations when you insert or update data in a table
with many rows. Additionally, the insert and update transactions are not
asynchronous. When inserting or updating a record, the transaction will not
complete until the indexes associated with the indexed fields of that record have
updated.  This slows down as the database grows in size.</p>

<p>We wanted the transactions between users, machines, and the Kitchen Sync to
be as asynchronous as possible. Our ability to take in data in RDF format would
not degrade with increasing amounts of data already taken in before warehousing
for analysis. Data exchange challenges between vendors and us included a few large
transactions in RDF format per data set, and how the length of the transaction
time depended solely on the speed of the network connection between the vendor
and our data center.</p>

<p>With the decision to use XML for storing poll metadata and RDF for storing
poll data in place, we turned our attention to the specifics of the persistence
layer.  We stored the poll objects in XML, as shown in this example:</p>

<pre><code>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;poll&gt;
    &lt;creator&gt;Fred Moyer&lt;/creator&gt;
    &lt;date&gt;2005-03-01&lt;/date&gt;
    &lt;vendor&gt;Voter Data Inc.&lt;/vendor&gt;
    &lt;location&gt;https://www.voterdatainc.com/poll/1234&lt;/location&gt;
    &lt;questions&gt;
        &lt;question&gt;
            &lt;name&gt;Who is buried in Grant's Tomb?&lt;/name&gt;
            &lt;answers&gt;
                &lt;answer&gt;
                    &lt;name&gt;Ulysses Grant&lt;/name&gt;
                    &lt;value&gt;0&lt;/value&gt;
                &lt;/answer&gt;
                &lt;answer&gt;
                    &lt;name&gt;John Kerry&lt;/name&gt;
                    &lt;value&gt;1&lt;/value&gt;
                &lt;/answer&gt;
                &lt;answer&gt;
                    &lt;name&gt;George Bush&lt;/name&gt;
                    &lt;value&gt;2&lt;/value&gt;
                &lt;/answer&gt;
                &lt;answer&gt;
                    &lt;name&gt;Alfred E.  Neumann&lt;/name&gt;
                    &lt;value&gt;3&lt;/name&gt;
                &lt;/answer&gt;
            &lt;/answers&gt;
        &lt;/question&gt;
    &lt;/questions&gt;
    &lt;media&gt;
        &lt;pdf&gt;
            &lt;name&gt;Name of a PDF file describing this poll&lt;/name&gt;
            &lt;raw&gt;The raw contents of the PDF file&lt;/raw&gt;
            &lt;text&gt;The text of the PDF file, generated with XPDF libs&lt;/text&gt;
        &lt;/pdf&gt;
    &lt;/media&gt;
&lt;/poll&gt;</code></pre>

<p>We also needed an API to manage those documents. We chose <a
href="http://www.sleepycat.com/products/xml.shtml">Berkeley DBXML</a> because
of its simple but effective API and its ability to scale to terabyte size if
needed. We created a poll class which subclassed the Sleepycat and XML::LibXML
modules and provided some Perlish methods for manipulating polls.</p>

<pre><code>package KSYNC::Model::Poll;

use strict;
use warnings;

use base qw(KSYNC::Model);
use SleepyCat::DbXml qw(simple);
use XML::LibXML;
use KSYNC::Exception;

my $ACTIVITY_LOC = 'data/poll.dbxml';

BEGIN {
    # Initialize the DbXml database
    my $container = XmlContainer-&gt;new($ACTIVITY_LOC);
}

# Call base class constructor KSYNC::Model-&gt;new
sub new {
    my ($class, %args) = @_;

    my $self = $class-&gt;SUPER::new(%args);
    return $self;
}

# Transform the poll object into an xml document
sub as_xml {
    my ($self, $id) = @_;
    
    my $dom = XML::LibXML::Document-&gt;new();
    my $pi = $dom-&gt;createPI( 'xml-styleshet', 
                             'href=&quot;/css/poll.xsl&quot; type=&quot;text/xsl&quot;' );
    $dom-&gt;appendChild($pi);
    my $element = XML::LibXML::Element-&gt;new('Poll');

    $element-&gt;appendTextChild('Type',        $self-&gt;type);
    $element-&gt;appendTextChild('Creator',     $self-&gt;creator);
    $element-&gt;appendTextChild('Description', $self-&gt;description);
    $element-&gt;appendTextChild('Vendor',      $self-&gt;vendor);
    $element-&gt;appendTextChild('Began',       $self-&gt;began);
    $element-&gt;appendTextChild('Completed',   $self-&gt;completed);

    my $questions = XML::LibXML::Element-&gt;new('Questions');

    for my $question ( @{ $self-&gt;{question} } ) {
        $questions-&gt;appendChild($question-&gt;as_element);
    }

    $element-&gt;appendChild($questions);

    $dom-&gt;setDocumentElement($element);
    return $dom;
}

sub save {
    my $self = shift;

    # Connect to the DbXml databae
    $container-&gt;open(Db::DB_CREATE);

    # Create a new document for storage from xml serialization of $self
    my $doc = XmlDocument-&gt;new();
    $doc-&gt;setContent($self-&gt;as_xml);
    
    # Save, throw an exception if problems happen
    eval { $container-&gt;putDocument($doc); };
    KSYNC::Exception-&gt;throw(&quot;Could not add document: $@&quot;) if $@;

    # Return the ID of the newly added document
    return $doc-&gt;getID();
}</code></pre>

<p>We chose RDF as the format for poll data because the format contains links
to resources that describe the namespaces of the document, making the document
self-describing. The availability of standardized namespaces such as <a
href="http://dublincore.org/">Dublin Core</a> gave us predefined tags such as
<code>dc:date</code> and <code>dc:creator</code>. We added our own namespaces
for representation of poll data. Depending on what verbosity of data the
vendors kept, we could add <code>dc:date</code> tags to different portions of
the document to provide historical references. We constructed our URLs in a <a
href="http://en.wikipedia.org/wiki/REST">REST</a> format for all web-based
resources.</p>

<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;rdf:RDF
	xmlns:RDF=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;
        xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot;
	xmlns:ourparty=&quot;http://www.ourparty.org/xml/schema#&quot;&gt;
	
	&lt;rdf:Description rdf:about=&quot;http://www.ourparty.org/poll/1234&quot;&gt;
	    &lt;dc:date&gt;2004-10-14&lt;/dc:date&gt;
            &lt;dc:creator&gt;fmoyer@plusthree.com&lt;/dc:creator&gt;
        &lt;/rdf:Description&gt;
        
        &lt;rdf:Bag&gt;
        &lt;rdf:li ourparty:id=&quot;6372095736&quot; ourparty:question=&quot;1&quot;
		    ourparty:answer=&quot;1&quot; dc:date=&quot;2005-03-01&quot; /&gt;
        &lt;rdf:li ourparty:id=&quot;2420080069&quot; ourparty:question=&quot;2&quot;
            ourparty:answer=&quot;3&quot; dc:date=&quot;2005-03-02&quot; /&gt;
	&lt;/rdf:Bag&gt;
&lt;/rdf:RDF&gt;</code></pre>

<p>We used <a href="http://search.cpan.org/perldoc?XML::SAX::Machines">SAX
machines</a> as drivers to generate summary models of RDF files and <a
href="http://search.cpan.org/perldoc?XML::LibXML">LibXML</a> streaming parsers
to traverse the RDF files.  We stacked drivers by using <a
href="http://search.cpan.org/perldoc?XML::SAX::Pipeline">pipelined SAX
machines</a> and constructed SAX drivers for the different vendor data schemas.
Cron-based machines scanned the RDF store, identified new poll data, and
processed them into summary XML documents which we served to administrative
users via XSLT transformations. Additionally, we used the SAX machines to
create denormalized SQL warehouses for data mining.</p>

<p>An example SAX driver for Voter Data, Inc. RDF poll data:</p>

<pre><code>package KSYNC::SAX::Voterdatainc;

use strict;
use warnings;

use base qw(KSYNC::SAX);

my %NS = (
    rdf      =&gt; 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',
    dc       =&gt; 'http://purl.org/dc/elements/1.1/',
    ourparty =&gt; 'http://www.ourparty.org/xml/schema#',
);

my $VENDOR = 'Voter Data, Inc.';

sub new {
    my $class = shift;

    # Call the super constructor to create the driver
    my $self = $class-&gt;SUPER::new(@_, { vendor =&gt; $VENDOR });

    return $self;
}

sub start_element {
    my ($self, $data) = @_;
    
    # Process rdf:li elements
    if ( $data-&gt;{Name} eq 'rdf:li' ) {
    
        # Grab the data
        my $id      = $data-&gt;{Attributes}{ &quot;{$NS{ourparty}}id&quot; }{Value};
        my $answer  = $data-&gt;{Attributes}{ &quot;{$NS{ourparty}}answer&quot; }{Value};
        my $creator = $data-&gt;{Attributes}{ &quot;{$NS{dc}}creator&quot; }{Value};
        my $date    = $data-&gt;{Attributes}{ &quot;{$NS{dc}}date&quot; }{Value};

        # Map the data to a common response
        $self-&gt;add_response({ vendor        =&gt; $VENDOR,
                              voter_id      =&gt; $id, 
                              support_level =&gt; $answer, 
                              creator       =&gt; $creator,
                              date          =&gt; $date,
                           });

        # Call the base class start_element method to do something with the data
        $self-&gt;SUPER::start_element($data);
}

1;</code></pre>

<p>We stored RDF documents compressed in bzip2 format, because bzip2
compression algorithm is especially efficient at compressing repeating element
data. As shown below in the SAX machine example, using <code>bzcat</code> as
the intake to a pipeline parser allowed decompression of the bzip2 documents for
parsing and creating a summary of a poll data set.</p>

<pre><code>#!/usr/bin/env perl

use strict;
use warnings;

use KSYNC::SAX::Voterdatainc;
use XML::SAX::Machines qw(Pipeline);

# The poll data
my $rdf = 'data/voterdatainc/1759265.rdf.bz2';

# Create a vendor specific driver
my $driver = KSYNC::SAX::Voterdatainc-&gt;new();

# Create a driver to add the data to a data warehouse handle
my $dbh = KSYNC::DBI-&gt;connect();
my $warehouser = KSYNC::SAX::DBI-&gt;new(
                    source =&gt; 'http://www.voterdatainc/ourparty/poll.xml',
                    dbh    =&gt; $dbh,
                );

# Create a parser which uncompresses the poll data set, summarizes it, and 
# outputs data to a filter which warehouses the denormalized data
my $parser = Pipeline(
                &quot;bzcat $rdf |&quot; =&gt;
                $driver        =&gt; 
                $warehouser    =&gt; 
;

# Parse the poll data
$parser-&gt;parse();

# Summarize the poll data
print &quot;Average support level:  &quot;,   $driver-&gt;average_support_level, &quot;\n&quot;;
print &quot;Starting date:  &quot;, 	    $driver-&gt;minimum_date, &quot;\n&quot;;
print &quot;Ending date:  &quot;, 	    $driver-&gt;maximum_date, &quot;\n&quot;;</code></pre>

<p>Between the polls, the XML Schema dictionaries, and the RDF files, we know
who the polls contacted, what they saw, and how they responded. A major benefit
of keeping the collected information in RDF format is the preservation of
historical information. We constructed SQL warehouses to analyze changes in
voter support levels over time.  This was critical for measuring the effect of
events such as presidential debates on voter interest and support.</p>

<p>Using RDF also provided us with the flexibility to map new data sources as
needed. If a vendor collected some information which we had not processed
before, they would add an <code>about</code> tag such as
<code>&lt;rdf:Description
rdf:about=&quot;http://www.datavendor.com/ourparty/poll5.xml&quot; /&gt;</code> , which
we would map to features of our SAX machines as needed.</p>

<p>We added some hooks to the SAX machines to match certain URIs and then
process selected element data. Late in the campaign, when early voting
started, we were able to quickly modify our existing SAX machines to collect
early voting data from the data streams and produce SQL warehouses for
analysis.</p>













<h3>Mechanization of Data Collection</h3>

<p>A major focus of the application was retrieving data from remote sources.
Certain vendors used our secure FTP site to send us data, but most had web and
FTP sites to which they posted the information. We needed a way to collect data
from those servers. Some vendors were able to provide data to us in XML and RDF
formats, but for the most part, we would receive data in CSV, TSV, or some
form of XML.  Each vendor generally had supplementary data beyond the normal
voter data fields which we also wanted to capture. Using that additional data
was not an immediate need, but by storing it in RDF format we could extract it
and generate SQL warehouses whenever necessary.</p>

<p>We developed a part of the application known as the spider and created a
database table containing information on the data source authentication,
protocol, and data structure details. A factory class
<code>KSYNC::Model::Spider</code> read the data source entries and constructed
spider objects for each data source. These spiders used <a
href="http://search.cpan.org/perldoc?Net::FTP">Net::FTP</a> and <a
href="http://search.cpan.org/perldoc?LWP">LWP</a> to retrieve poll data, and
processed the data using the appropriate <code>KSYNC::SAX</code> machine. To
add a new data source to our automated collection system, an entry in the
database configured the spider, and if the new data source had data in a format
that we did not support, we added a SAX machine for that data source.</p>

<p>An example of spider usage:</p>

<pre><code>package KSYNC::Model::Spider;

use strict;
use warnings;

use Carp 'croak';
use base 'KSYNC::Model';

sub new {
    my ($class, %args) = @_;
    
    # Create an FTP or HTTP spider based on the type specified in %args
    my $spider_pkg = $class-&gt;_factory($args{type});
    my $self = $spider_pkg-&gt;new(%args);

    return $self;
}

sub _factory {
    my ($class, $type) = @_;

    # Create the package name for the spider type
    my $pkg = join '::', $class, $type;
    
    # Load the package
    eval &quot;use $pkg&quot;;
    croak(&quot;Error loading factory module: $@&quot;) if $@;

    return $pkg;
}

1;

package KSYNC::Model::Spider::FTP;

use Net::FTP;
use KSYNC::Exception;

sub new {
    my ($class, %args) = @_;
    
    my $self = { %args };

    # Load the appropriate authentication package via Spider::Model::Auth 
    # factory class
    $self-&gt;{auth} = Spider::Model::Auth-&gt;new(%{$args{auth}});

    return bless $self, $class;
}

sub authenticate {
    my $self = shift;
    
    # Login
    eval { $self-&gt;ftp-&gt;login($self-&gt;auth-&gt;username, $self-&gt;auth-&gt;password); };
     
    # Throw an exception if problems occurred
    KSYNC::Exception-&gt;throw(&quot;Cannot login &quot;, $self-&gt;ftp-&gt;message) if $@;
}

sub crawl {
    my $self = shift;
    
    # Set binary retrieval mode
    $self-&gt;ftp-&gt;binary;

    # Find new poll data
    my @datasets = $self-&gt;_find_new();

    # Process that poll data
    foreach my $dataset (@datasets) {
        eval { $self-&gt;_process($dataset); };
        $self-&gt;error(&quot;Could not process poll data $dataset-&gt;id&quot;, $@) if $@;
    }
}

sub ftp { 
    croak(&quot;Method Not Implemented!&quot;) if @_ &gt; 1; 
    $_[0]-&gt;{ftp} ||= Net::FTP-&gt;new($self-&gt;auth-&gt;host); 
}

1;

#!/usr/bin/env perl

use strict;
use warnings;

use KSYNC::Model::Spider;
use KSYNC::Model::Vendor;

# Retrieve a vendor so we can grab their latest data
my $vendor = KSYNC::Model::Vendor-&gt;retrieve({ 
  name =&gt; 'Voter Data, Inc.',
});

# Construct a spider to crawl their site
my $spider = KSYNC::Model::Spider-&gt;new({ type =&gt; $vendor-&gt;type });

# Login
$spider-&gt;login();

# Grab the data
$spider-&gt;crawl();

# Logout
$spider-&gt;logout();

1;</code></pre>

<h3>Conclusions</h3>

<p>In this project, getting things done was of paramount importance.  Perl
allowed us to deal with the complexities of the business requirements and the
technical details of data schemas and formats without presenting additional
technical obstacles, as programming languages occasionally do. The <a
href="http://www.cpan.org">CPAN</a>, <a
href="http://perl.apache.org">mod_perl</a>, and <a
href="http://httpd.apache.org/apreq">libapreq</a> provided the components that
allowed us to quickly build an application to deal with complex, semi-structured
data on an enterprise scale. From creating a user friendly web application to
automating data collection and SQL warehouse generation, Perl was central to
the success of this project.</p>

<h3>Credits</h3>

<p>Thanks to the following people who made this possible and contributed to
this project: Thomas Burke, Charles Frank, Lyle Brooks, Lina Brunton, Aaron
Ross, Alan Julson, Marc Schloss, and Robert Vadnais.</p>

<p>Thanks to <a href="http://www.plusthree.com/">Plus Three LP</a> for
sponsoring work on this project.</p>



        </div>



    </div>
    <div class="asset-footer"></div>
</div>


                            
                            <div id="entry-742" class="entry-asset asset hentry">
    <div class="asset-header">
        <h2 class="asset-name entry-title"><a href="/pub/2005/03/lightning2.html" rel="bookmark">More Lightning Articles</a></h2>
        <div class="asset-meta">
            <span class="byline">
    
                By <span class="vcard author">chromatic</span> on <abbr class="published" title="2005-03-31T00:00:00-08:00">March 31, 2005 12:00 AM</abbr>
    
            </span>

            
            

        </div>
    </div>
    <div class="asset-content entry-content">

        <div class="asset-body">
            
<!-- sidebar begins -->
<!-- don't move sidebars-->
<!-- sidebar ends -->
<h3><a name="emacsperl">Customizing Emacs with Perl</a></h3>

<p>by Bob DuCharme</p>

<p>Over time, I've accumulated a list of Emacs customizations I wanted to
implement when I got the chance. For example, I'd like macros to perform
certain global replaces just within a marked block, and I'd like a macro to
reformat an Outlook formatted date to an ISO 8609 formatted date. I'm not
overly intimidated by the elisp language used to customize Emacs behavior; I've
copied elisp code and modified it to make some tweaks before, I had a healthy
dose of Scheme and LISP programming in school, and I've done extensive work
with XSLT, a descendant of these grand old languages. Still, as with a lot of
postponed editor customization work, I knew I'd have to use these macros many,
many times before they earned back the time invested in creating them, because
I wasn't that familiar with string manipulation and other basic operations in a
LISP-based language. I kept thinking to myself, "This would be so easy if I
could just do the string manipulation in Perl!"</p>

<p>Then, I figured out how I could write Emacs functions that called Perl to
operate on a marked block (or, in Emacs parlance, a "region"). Many Emacs users
are familiar with the <code>Escape+|</code> keystroke, which invokes the
<code>shell-command-on-region</code> function. It brings up a prompt in the
minibuffer where you enter the command to run on the marked region, and after
you press the Enter key Emacs puts the command's output in the minibuffer if it
will fit, or into a new "*Shell Command Output*" buffer if not. For example,
after you mark part of an HTML file you're editing as the region, pressing
<code>Escape+|</code> and entering <code>wc</code> (for "word count") at the
minibuffer's "Shell command on region:" prompt will feed the text to this
command line utility if you have it in your path, and then display the number of
lines, words, and characters in the region at the minibuffer. If you enter
<code>sort</code> at the same prompt, Emacs will run that command instead of
<code>wc</code> and display the result in a buffer.</p>

<p>Entering <code>perl /some/path/foo.pl</code> at the same prompt will run the
named Perl script on the marked region and display the output appropriately.
This may seem like a lot of keystrokes if you just want to do a global replace
in a few paragraphs, but remember: <code>Ctrl+|</code> calls Emacs's built-in
<code>shell-command-on-region</code> function, and you can call this same
function from a new function that you define yourself. My recent great
discovery was that along with parameters identifying the region boundaries and
the command to run on the region, <code>shell-command-on-region</code> takes an
optional parameter that lets you tell it to replace the input region with the
output region. When you're editing a document with Emacs, this allows you to
pass a marked region outside of Emacs to a Perl script, let the Perl script do
whatever you like to the text, and then Emacs will replace the original text
with the processed version. (If your Perl script mangled the text, Emacs'
excellent <code>undo</code> command can come to the rescue.)</p>

<p>Consider an example. When I take notes about a project at work, I might
write that Joe R. sent an e-mail telling me that a certain system won't need any
revisions to handle the new data. I want to make a note of when he told me
this, so I copy and paste the date from the e-mail he sent. We use Microsoft
Outlook at work, and the dates have a format following the model "Tue 2/22/2005
6:05 PM". I already have an Emacs macro bound to <code>alt+d</code> to insert
the current date and time (also handy when taking notes) and I wanted the date
format that refers to e-mails to be the same format as the ones inserted with
my <code>alt+d</code> macro: an ISO 8609 format of the form
"2005-02-22T18:05".</p>

<p>The <em>.emacs</em> startup file holds customized functions that you want
available during your Emacs session. The following shows a bit of code that I
put in mine so that I could convert these dates:</p>

<pre><code>(defun OLDate2ISO ()
  (interactive)
  (shell-command-on-region (point)
         (mark) "perl c:/util/OLDate2ISO.pl" nil t))</code></pre>

<p>The <code>(interactive)</code> declaration tells Emacs that the function
being defined can be invoked interactively as a command. For example, I can
enter "OLDate2ISO" at the Emacs minibuffer command prompt, or I can press a
keystroke or select a menu choice bound to this function. The
<code>point</code> and <code>mark</code> functions are built into Emacs to
identify the boundaries of the currently marked region, so they're handy for
the first and second arguments to <code>shell-command-on-region</code>, which
tell it which text is the region to act on. The third argument is the actual
command to execute on the region; enter any command available on your operating
system that can accept standard input. To define your own Emacs functions that
call Perl functions, just change the script name in this argument from
<code>OLDate2ISO</code> to anything you like and then change this third
argument to <code>shell-command-on-region</code> to call your own Perl
script.</p>

<p>Leave the last two arguments as <code>nil</code> and <code>t</code>.  Don't
worry about the fourth parameter, which controls the buffer where the shell
output appears. (Setting it to <code>nil</code> means "don't bother.") The
fifth parameter is the key to the whole trick: when non-nil, it tells Emacs to
replace the marked text in the editing buffer with the output of the command
described in the third argument instead of sending the output to a buffer.</p>

<p>If you're familiar with Perl, there's nothing particularly interesting about
the <em>OLDate2ISO.pl</em> script. It does some regular expression matching to
split up the string, converts the time to a 24 hour clock, and rearranges the
pieces:</p>

<pre><code># Convert Outlook format date to ISO 8309 date 
#(e.g. Wed 2/16/2005 5:27 PM to 2005-02-16T17:27)
while (&lt;&gt;) {
  if (/\w+ (\d+)\/(\d+)\/(\d{4}) (\d+):(\d+) ([AP])M/) {
     $AorP = $6;
     $minutes = $5;
     $hour = $4;
     $year = $3;
     $month = $1;
     $day = $2;
     $day = '0' . $day if ($day &lt; 10);
     $month = '0' . $month if ($month &lt; 10);
     $hour = $hour + 12 if ($6 eq 'P');
     $hour = '0' . $hour if ($hour &lt; 10);
     $_ = "$year-$month-$day" . "T$hour:$minutes";
  }
  print;
}</code></pre>

<p>When you start up Emacs with a function definition like the <code>defun
OLDate2ISO</code> one shown above in your <em>.emacs</em> file, the function is
available to you like any other in Emacs. Press <code>Escape+x</code> to bring
up the Emacs minibuffer command line and enter "OLDate2ISO" there to execute it
on the currently marked buffer. Like any other interactive command, you can
also assign it to a keystroke or a menu choice.</p>

<p>There might be a more efficient way to do the Perl coding shown above, but I
didn't spend too much time on it. That's the beauty of it: with five minutes of
Perl coding and one minute of elisp coding, I had a new menu choice to quickly
do the transformation I had always wished for.</p>

<p>Another example of something I always wanted is the following
<em>txt2htmlp.pl</em> script, which is useful after plugging a few paragraphs
of plain text into an HTML document:</p>

<pre><code># Turn lines of plain text into HTML p elements.
while (&lt;&gt;) {
  chop($_);
  # Turn ampersands and &lt; into entity references.
  s/\&amp;/\&amp;amp\;/g;
  s/&lt;/\&amp;lt\;/g;
  # Wrap each non-blank line in a "p" element.
  print "&lt;p&gt;$_&lt;/p&gt;\n\n" if (!(/^\s*$/));
}</code></pre>

<p>Again, it's not a particularly innovative Perl script, but with the
following bit of elisp in my <em>.emacs</em> file, I have something that
greatly speeds up the addition of hastily written notes into a web page,
especially when I create an Emacs menu choice to call this function:</p>

<pre><code>(defun txt2htmlp ()
  (interactive)
  (shell-command-on-region (point) 
         (mark) "perl c:/util/txt2htmlp.pl" nil t))</code></pre>

<p>Sometimes when I hear about hot new editors, I wonder whether they'll ever
take the place of Emacs in my daily routine. Now that I can so easily add the
power of Perl to my use of Emacs, it's going to be a lot more difficult for any
other editor to compete with Emacs on my computer.</p>













<h3><a name="linetrace">Debug Your Programs with Devel::LineTrace</a></h3>

<p>by Shlomi Fish</p>

<p>Often, programmers find a need to use print statements to output information
to the screen, in order to help them analyze what went wrong in running the
script. However, including these statements verbatim in the script is not such
a good idea. If not promptly removed, these statements can have all kinds of
side-effects: slowing down the script, destroying the correct format of its
output (possibly ruining test-cases), littering the code, and confusing the
user. It would be a better idea not to place them within the code in the first
place. How, though, can you debug without debugging?</p>

<p>Enter <a
href="http://search.cpan.org/dist/Devel-LineTrace/">Devel::LineTrace</a>, a
Perl module that can assign portions of code to execute at arbitrary lines
within the code. That way, the programmer can add print statements in relevant
places in the code without harming the program's integrity.</p>

<h4>Verifying That <code>use lib</code> Has Taken Effect</h4>

<p>One example I recently encountered was that I wanted to use a module I wrote
from the specialized directory where I placed it, while it was already installed
in the Perl's global include path. I used a <code>use lib "./MyPath"</code>
directive to make sure this was the case, but now had a problem.  What if there
was a typo in the path of the <code>use lib</code> directive, and as a result,
Perl loaded the module from the global path instead? I needed a way to verify
it.</p>

<p>To demonstrate how <code>Devel::LineTrace</code> can do just that, consider
a similar script that tries to use a module named <code>CGI</code> from the
path <em>./MyModules</em> instead of the global Perl path. (It is a bad idea to
name your modules after names of modules from CPAN or from the Perl
distribution, but this is just for the sake of the demonstration.)</p>

<pre><code>#!/usr/bin/perl -w

use strict;
use lib "./MyModules";

use CGI;

my $q = CGI-&gt;new();

print $q-&gt;header();</code></pre>

<p>Name this script <em>good.pl</em>. To test that Perl loaded the <code>CGI</code> module
from the <em>./MyModules</em> directory, direct <code>Devel::LineTrace</code> to print the
relevant entry from the <code>%INC</code> internal variable, at the first line
after the <code>use CGI</code> one.</p>

<p>To do so, prepare this file and call it <em>test-good.txt</em>:</p>

<pre><code>good.pl:8
    print STDERR "\$INC{CGI.pm} == ", $INC{"CGI.pm"}, "\n";</code></pre>

<p>Place the file and the line number at which the trace should be inserted on the first line.
Then comes the code to evaluate, indented from the start of the line. After the
first trace, you can put other traces, by starting the line with the filename
and line number, and putting the code in the following (indented) lines.  This
example is simple enough not to need that though.</p>

<p>After you have prepared <em>test-good.txt</em>, run the script
through <code>Devel::LineTrace</code> by executing the following command:</p>

<pre><code>$ PERL5DB_LT="test-good.txt" perl -d:LineTrace good.pl</code></pre>

<p>(This assumes a Bourne-shell derivative.). The <code>PERL5DB_LT</code>
environment variable contains the path of the file to use for debugging, and
the <code>-d:LineTrace</code> directive to Perl instructs it to debug the
script through the <code>Devel::LineTrace</code> package.</p>

<p>As a result, you should see either the following output to standard
error:</p>

<pre><code>$INC{CGI.pm} == MyModules/CGI.pm</code></pre>

<p>meaning that Perl indeed loaded the module from the <em>MyModules</em>
sub-directory of the current directory. Otherwise, you'll see something
like:</p>

<pre><code>$INC{CGI.pm} == /usr/lib/perl5/vendor_perl/5.8.4/CGI.pm</code></pre>

<p>...which means that it came from the global path and something went wrong.</p>

<h4>Limitations of <code>Devel::LineTrace</code></h4>

<p><code>Devel::LineTrace</code> has two limitations:</p>

<ol>

<li>Because it uses the Perl debugger interface and stops at every line (to
check whether it contains a trace), program execution is considerably slower
when the program is being run under it.</li>

<li>It assigns traces to line numbers, and therefore you must update it if the
line numbering of the file changes.</li>

</ol>

<p>Nevertheless, it is a good solution for keeping those pesky
<code>print</code> statements out of your programs. Happy LineTracing!</p>

<h3><a name="mockdbi">Using Test::MockDBI</a></h3>

<p>by Mark Leighton Fisher</p>

<p>What if you could test your program's use of the DBI just by creating a set
of rules to guide the DBI's behavior&#8212;without touching a database (unless you
want to)?  That is the promise of <a
href="http://search.cpan.org/perldoc?Test::MockDBI">Test::MockDBI</a>, which by
mocking-up the entire DBI API gives you unprecedented control over every aspect
of the DBI's interface with your program.</p>

<p><code>Test::MockDBI</code> uses <a
href="http://search.cpan.org/perldoc?Test::MockObject::Extends">Test::MockObject::Extends</a>
to mock all of the DBI transparently.  The rest of the program knows nothing
about using <code>Test::MockDBI</code>, making <code>Test::MockDBI</code> ideal for testing programs that
you are taking over, because you only need to add the <code>Test::MockDBI</code> invocation code&#8212;
you do not have to modify any of the other program code.  (I have found this
very handy as a consultant, as I often work on other people's code.)</p>

<p>Rules are invoked when the current SQL matches the rule's SQL pattern.  For
finer control, there is an optional numeric DBI testing type for each rule, so
that a rule only fires when the SQL matches <em>and</em> the current DBI
testing type is the specified DBI testing type.  You can specify this numeric
DBI testing type (a simple integer matching <code>/^\d+$/</code>) from the
command line or through <code>Test::MockDBI::set_dbi_test_type()</code>.  You
can also set up rules to fail a transaction if a specific
<code>DBI::bind_param()</code> parameter is a specific value.  This means there
are three types of conditions for <code>Test::MockDBI</code> rules:</p>

<ul>

<li>The current SQL</li>
<li>The current DBI testing type</li>
<li>The current <code>bind_param()</code> parameter values</li>

</ul>

<p>Under <code>Test::MockDBI</code>, <code>fetch*()</code> and <code>select*()</code>
methods default to returning nothing (the empty array, the empty hash, or undef
for scalars).  <code>Test::MockDBIM</code> lets you take control of their returned data with
the methods <code>set_retval_scalar()</code> and
<code>set_retval_array()</code>.  You can specify the returned data directly in
the <code>set_retval_*()</code> call, or pass a CODEREF that generates a return
value to use for each call to the matching <code>fetch*()</code> or
<code>select*()</code> method.  CODEREFs let you both simulate DBI's
interaction with the database more accurately (as you can return a few rows,
then stop), and add in any kind of state machine or other processing
needed to precisely test your code.</p>

<p>When you need to test that your code handles database or DBI failures,
<code>bad_method()</code> is your friend.  It can fail any DBI method, with the
failures dependent on the current SQL and (optionally) the current DBI testing
type.  This capability is necessary to test code that handles bad database
<code>UPDATE</code>s, <code>INSERT</code>s, or <code>DELETE</code>s, along with
being handy for testing failing <code>SELECT</code>s.</p>

<p><code>Test::MockDBI</code> extends your testing capabilities to testing code that is
difficult or impossible to test on a live, working database.  <code>Test::MockDBI's</code>
mock-up of the entire DBI API lets you add <code>Test::MockDBI</code> to your programs
without having to modify their current DBI code.  Although it is not finished
(not all of the DBI is mocked-up yet), <code>Test::MockDBI</code> is already a powerful tool
for testing DBI programs.</p>













<h3><a href="#unbuffering">Unnecessary Unbuffering</a></h3>

<p>by chromatic</p>

<p>A great joy in a programmer's life is removing useless code, especially
when its absence improves the program. Often this happens in old codebases
or codebases thrown together hastily. Sometimes it happens in code written
by novice programmers who try several different ideas all together and fail
to undo their changes.</p>

<p>One such persistent idiom is wholesale, program-wide unbuffering, which
can take the form of any of:</p>

<pre><code>local $| = 1;
$|++;
$| = 1;</code></pre>

<p>Sometimes this is valuable. Sometimes it's vital. It's not the default
for very good reason, though, and at best, including one of these lines in
your program is useless code.</p>

<h4>What's Unbuffering?</h4>

<p>By default, modern operating systems don't send information to output
devices directly, one byte at a time, nor do they read information from
input devices directly, one byte at a time. IO is so slow, especially for
networks, compared to processors and memory that adding buffers and trying
to fill them before sending and receiving information can improve
performance.</p>

<p>Think of trying to fill a bathtub from a hand pump. You <em>could</em>
pump a little water into a bucket and walk back and forth to the bathtub,
or you could fill a trough at the pump and fill the bucket from the trough.
If the trough is empty, pumping a little bit of water into the bucket will
give you a faster start, but it'll take longer in between bucket loads than
if you filled the trough at the start and carried water back and forth
between the trough and the bathtub.</p>

<p>Information isn't exactly like water, though. Sometimes it's more
important to deliver a message immediately even if it doesn't fill up a
bucket. "Help, fire!" is a very short message, but waiting to send it when
you have a full load of messages might be the wrong thing.</p>

<p>That's why modern operating systems also let you unbuffer specific
filehandles. When you print to an unbuffered filehandle, the operating
system will handle the message immediately. That doesn't guarantee that
whoever's on the other side of the handle will respond immediately; there
might be a pump and a trough there.</p>

<h4>What's the Damage?</h4>

<p>According to Mark-Jason Dominus' <a
href="http://perl.plover.com/FAQs/Buffering.html">Suffering from
Buffering?</a>, one sample showed that buffered reading was 40% faster than
unbuffered reading, and buffered writing was 60% faster. The latter number may
only improve when considering network communications, where the overhead of
sending and receiving a single packet of information can overwhelm short
messages.</p>

<p>In simple interactive applications though, there may be no benefit. When
attached to a terminal, such as a command line, Perl operates in
line-buffered mode. Run the following program and watch the output
carefully:</p>

<pre><code>#!/usr/bin/perl

use strict;
use warnings;

# buffer flushed at newline
loop_print( 5, "Line-buffered\n" );

# buffer not flushed until newline
loop_print( 5, "Buffered  " );
print "\n";

# buffer flushed with every print
{
    local $| = 1;
    loop_print( 5, "Unbuffered  " );
}

sub loop_print
{
    my ($times, $message) = @_;

    for (1 .. $times)
    {
        print $message;
        sleep 1;
    }
}</code></pre>

<p>The first five greetings appear individually and immediately.  Perl flushes
the buffer for STDOUT when it sees the newlines.  The second set appears after
five seconds, all at once, when it sees the newline after the loop. The third
set appears individually and immediately because Perl flushes the buffer after
every <code>print</code> statement.</p>

<p>Terminals are different from everything else, though. Consider the case of
writing to a file. In one terminal window, create a file named
<em>buffer.log</em> and run <code>tail -f buffer.log</code> or its equivalent
to watch the growth of the file in real time. Then add the following lines to
the previous program and run it again:</p>

<pre><code>open( my $output, '&gt;', 'buffer.log' ) or die "Can't open buffer.log: $!";
select( $output );
loop_print( 5, "Buffered\n" );
{
      local $| = 1;
      loop_print( 5, "Unbuffered\n" );
}</code></pre>

<p>The first five messages appear in the log in a batch, all at once, even
though they all have newlines.  Five messages aren't enough to fill the buffer.
Perl only flushes it when it unbuffers the filehandle on assignment to
<code>$|</code>.  The second set of messages appear individually, one second
after another.</p>

<p>Finally, the STDERR filehandle is hot by default. Add the following lines to
the previous program and run it yet again:</p>

<pre><code>select( STDERR );
loop_print( 5, "Unbuffered STDERR " );</code></pre>

<p>Though no code disables the buffer on STDERR, the five messages should print
immediately, just as in the other unbuffered cases. (If they don't, your OS is
weird.)</p>

<h4>What's the Solution?</h4>

<p>Buffering exists for a reason; it's almost always the right thing to do.
When it's the wrong thing to do, you can disable it. Here are some rules of
thumb:</p>

<ul>

<li>Never disable buffering by default.</li>

<li>Disable buffering when and while you have multiple sources writing to
the same output and their order matters.</li>

<li>Never disable buffering for network outputs by default.</li>

<li>Disable buffering for network outputs only when the expected time
between full buffers exceeds the expected client timeout length.</li>

<li>Don't disable buffering on terminal outputs. For STDERR, it's useless,
dead code. For STDOUT, you probably don't need it.</li>

<li>Disable buffering if it's more important to print messages regularly
than efficiently.</li>

<li>Don't disable buffering until you know that the buffer is a
problem.</li>

<li>Disable buffering in the smallest scope possible.</li>

</ul>
<!-- sidebar begins -->
 <csperl file="grab" domain="on" record="b/1518" template="b/article_sidebar2.view">
<!-- sidebar ends -->



        </div>



    </div>
    <div class="asset-footer"></div>
</div>




                            <div class="content-nav">
                                <a href="/pub/data-structures/">&laquo; Data Structures</a> |
                                <a href="/pub/">Main Index</a> |
                                <a href="/pub/archives.html">Archives</a>
                                | <a href="/pub/debugging/">Debugging &raquo;</a>
                            </div>


                        </div>
                    </div>


                    <div id="beta">
    <div id="beta-inner">


    
    <div class="widget-what-is-perl widget">
    <div class="widget-content widget-content-what-is-perl">
       Visit the home of the  Perl programming language: <a href="http://www.perl.org/">Perl.org</a
    </div>
</div>
<div class="widget-find-out-more widget-archives widget">
    <div class="widget-content">
        <ul>
            <li><a href="http://www.perl.org/get.html">Download</a></li>
            <li><a href="http://perldoc.perl.org/">Documentation</a></li>
            <li><a href="http://blogs.perl.org/">Perl Bloggers</a></li>
            <li><a href="http://news.perlfoundation.org/">Foundation News</a></li>
        </ul>
    </div>
</div><div class="widget-tcpc widget">
<h3 class="widget-header">Sponsored by</h3>
    <div class="widget-content">
        <a href="http://training.perl.com/" alt="Perl Training" target="_blank"><img src="/i/tcpc.png" width="150" height="50"></a>
    </div>
</div>

<div class="widget-syndication widget">
    <div class="widget-content">
        <ul>
            <li><img src="/mt-static/images/status_icons/feed.gif" alt="Subscribe to feed" width="9" height="9" /> <a href="/pub/atom.xml">Subscribe to this website's feed</a></li>

        </ul>
    </div>
</div>
<div class="widget-powered widget">
    <div class="widget-content">
        <a href="http://www.movabletype.com/"><img src="/mt-static/images/bug-pbmt-white.png" alt="Powered by Movable Type 5.02" width="120" height="75" /></a>
    </div>
</div>



    </div>
</div>






                </div>
            </div>


            <div id="footer">
    <div id="footer-inner">
        <div id="footer-content">
            <div class="widget-powered widget">
                <div class="widget-content">
                    Powered by <a href="http://www.movabletype.com/" rel="generator">Movable Type Pro</a>
                </div>
            </div>

        </div>
    </div>
</div>



        </div>
    </div>
</body>
</html>
