<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" id="sixapart-standard">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="generator" content="Movable Type Pro 5.02" />
<link rel="stylesheet" href="/pub/styles.css" type="text/css" />
<link rel="start" href="/pub/" title="Home" />
<link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/pub/atom.xml" />
<script type="text/javascript" src="/pub/mt.js"></script>

<script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-50555-22']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type =
   'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
   'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0];
   s.parentNode.insertBefore(ga, s);
 })();

</script>
    <title>Perl.com: Troubleshooting Archives</title>


</head>
<body id="perl-com" class="mt-archive-listing mt-category-archive layout-wt">
    <div id="container">
        <div id="container-inner">


            <div id="header">
    <div id="header-inner">
        <div id="header-content">
        <div id="top_advert"> 
<!-- Put any landscape advert in here -->
<a href="http://www.perlfoundation.org/" target="_new">
<img src="/i/tpf_banner.png" width="468" height="60" /></a>
        </div> 



            <div id="header-name"><a href="/pub/" accesskey="1">Perl.com</a></div>
            <div id="header-description"></div>




        </div>
    </div>
</div>



            <div id="content">
                <div id="content-inner">


                    <div id="alpha">
                        <div id="alpha-inner">

                            
                            <h1 id="page-title" class="archive-title">Recently in <em>Troubleshooting</em> Category</h1>






                            
                            <div id="entry-798" class="entry-asset asset hentry">
    <div class="asset-header">
        <h2 class="asset-name entry-title"><a href="/pub/2005/07/bestpractices.html" rel="bookmark">Ten Essential Development Practices</a></h2>
        <div class="asset-meta">
            <span class="byline">
    
                By <span class="vcard author">Damian Conway</span> on <abbr class="published" title="2005-07-14T00:00:00-08:00">July 14, 2005 12:00 AM</abbr>
    
            </span>

            
            

        </div>
    </div>
    <div class="asset-content entry-content">

        <div class="asset-body">
            
<!-- sidebar begins -->
<!-- don't move sidebars-->
<!-- sidebar ends -->
<br clear="all" />
<p>The following ten tips come from <em><a
href="http://www.oreilly.com/catalog/perlbp/">Perl Best Practices</a></em>, a
new book of Perl coding and development guidelines by Damian Conway.</p>

<h3>1. Design the Module's Interface First</h3>

<p>The most important aspect of any module is not how it implements the
facilities it provides, but the way in which it provides those facilities in
the first place. If the module's API is too awkward, or too complex, or too
extensive, or too fragmented, or even just poorly named, developers will avoid
using it. They'll write their own code instead. In that way, a poorly designed
module can actually reduce the overall maintainability of a system.</p>

<p>Designing module interfaces requires both experience and creativity. Perhaps
the easiest way to work out how an interface should work is to "play test" it:
to write examples of code that will use the module before implementing the
module itself. These examples will not be wasted when the design is complete.
You can usually recycle them into demos, documentation examples, or the core of
a test suite.</p>

<a href="http://conferences.oreillynet.com/os2005/"><img src="http://conferences.oreillynet.com/images/os2005/banners/120x240.gif" width="120" height="240" hspace="10" vspace="10" border="0" alt="O'Reilly Open Source Convention 2005." align="right" /></a>

<p>The key, however, is to write that code as if the module were already
available, and write it the way you'd most like the module to work.</p>

<p>Once you have some idea of the interface you want to create, convert your
"play tests" into actual tests (see Tip #2). Then it's just a Simple Matter Of
Programming to make the module work the way that the code examples and the
tests want it to.</p>

<p>Of course, it may not be possible for the module to work the way you'd most
like, in which case attempting to implement it that way will help you determine
what aspects of your API are not practical, and allow you to work out what
might be an acceptable alternative.</p>

<h3>2. Write the Test Cases Before the Code</h3>

<p>Probably the single best practice in all of software development is writing
your test suite first.</p>

<p>A test suite is an executable, self-verifying specification of the behavior
of a piece of software. If you have a test suite, you can--at any point in the
development process--verify that the code works as expected. If you have a test
suite, you can--after any changes during the maintenance cycle--verify that the
code still works as expected.</p>

<p>Write the tests first. Write them as soon as you know what your interface
will be (see #1). Write them before you start coding your application or
module.  Unless you have tests, you have no unequivocal specification of what
the software should do, and no way of knowing whether it does it.</p>

<p>Writing tests always seems like a chore, and an unproductive chore at that:
you don't have anything to test yet, so why write tests? Yet most developers
will--almost automatically--write driver software to test their new module in
an ad hoc way:<a id="OLE_LINK428"></a></p>

<pre><code>&gt; cat try_inflections.pl

# Test my shiny new English inflections module...

use Lingua::EN::Inflect qw( inflect );

# Try some plurals (both standard and unusual inflections)...

my %plural_of = (
   'house'         =&gt; 'houses',
   'mouse'         =&gt; 'mice',
   'box'           =&gt; 'boxes',
   'ox'            =&gt; 'oxen',
   'goose'         =&gt; 'geese',
   'mongoose'      =&gt; 'mongooses', 
   'law'           =&gt; 'laws',
   'mother-in-law' =&gt; 'mothers-in-law',
);
 
# For each of them, print both the expected result and the actual inflection...

for my $word ( keys %plural_of ) {
   my $expected = $plural_of{$word};
   my $computed = inflect( "PL_N($word)" );
 
   print "For $word:\n", 
         "\tExpected: $expected\n",
         "\tComputed: $computed\n";
}</code></pre>

<p>A driver like that is actually harder to write than a test suite, because
you have to worry about formatting the output in a way that is easy to read.
It's also much harder to use the driver than it would be to use a test suite,
because every time you run it you have to wade though that formatted output
and verify "by eye" that everything is as it should be. That's also
error-prone; eyes are not optimized for picking out small differences in the
middle of large amounts of nearly identical text.</p>

<p>Instead of hacking together a driver program, it's easier to write a test
program using the standard <a
href="http://search.cpan.org/perldoc?Test::Simple">Test::Simple</a> module.
Instead of <code>print</code> statements showing what's being tested, you just
write calls to the <code>ok()</code> subroutine, specifying as its first
argument the condition under which things are okay, and as its second argument
a description of what you're actually testing:<a id="OLE_LINK429"></a></p>

<pre><code>&gt; cat inflections.t

use Lingua::EN::Inflect qw( inflect);

use Test::Simple qw( no_plan);

my %plural_of = (
   'mouse'         =&gt; 'mice',
   'house'         =&gt; 'houses',
   'ox'            =&gt; 'oxen',
   'box'           =&gt; 'boxes',
   'goose'         =&gt; 'geese',
   'mongoose'      =&gt; 'mongooses', 
   'law'           =&gt; 'laws',
   'mother-in-law' =&gt; 'mothers-in-law',
);

for my $word ( keys %plural_of ) {
   my $expected = $plural_of{$word};
   my $computed = inflect( "PL_N($word)" );

   ok( $computed eq $expected, "$word -&gt; $expected" );
}</code></pre> 













<p>Note that this code loads <code>Test::Simple</code> with the argument
<code>qw( no_plan )</code>. Normally that argument would be <code>tests =&gt;
count</code>, indicating how many tests to expect, but here the tests are
generated from the <code>%plural_of</code> table at run time, so the final
count will depend on how many entries are in that table. Specifying a fixed
number of tests when loading the module is useful if you happen know that
number at compile time, because then the module can also "meta-test:" verify
that you carried out all the tests you expected to.</p>

<p>The <code>Test::Simple</code> program is slightly more concise and readable
than the original driver code, and the output is much more compact and
informative:</p>

<pre><code>&gt; perl inflections.t

ok 1 - house -&gt; houses
ok 2 - law -&gt; laws
not ok 3 - mongoose -&gt; mongooses
#     Failed test (inflections.t at line 21)
ok 4 - goose -&gt; geese
ok 5 - ox -&gt; oxen
not ok 6 - mother-in-law -&gt; mothers-in-law
#     Failed test (inflections.t at line 21)
ok 7 - mouse -&gt; mice
ok 8 - box -&gt; boxes
1..8
# Looks like you failed 2 tests of 8. </code></pre>

<p>More importantly, this version requires far less effort to verify the
correctness of each test. You just scan down the left margin looking for a
<code>not</code> and a comment line.</p>

<p>You might prefer to use the <a
href="http://search.cpan.org/perldoc?Test::More">Test::More</a> module instead
of <code>Test::Simple</code>. Then you can specify the actual and expected
values separately, by using the <code>is()</code> subroutine, rather than
<code>ok()</code>:</p>

<pre><code>use Lingua::EN::Inflect qw( inflect );
use Test::More qw( no_plan ); # Now using more advanced testing tools

my %plural_of = (
   'mouse'         =&gt; 'mice',
   'house'         =&gt; 'houses',
   'ox'            =&gt; 'oxen',
   'box'           =&gt; 'boxes',
   'goose'         =&gt; 'geese',
   'mongoose'      =&gt; 'mongooses', 
   'law'           =&gt; 'laws',
   'mother-in-law' =&gt; 'mothers-in-law',
);

for my $word ( keys %plural_of ) {
   my $expected = $plural_of{$word};
   my $computed = inflect( "PL_N($word)" );

   # Test expected and computed inflections for string equality...
   is( $computed, $expected, "$word -&gt; $expected" );
}</code></pre> 

<p>Apart from no longer having to type the <code>eq</code> yourself, this
version also produces more detailed error messages:</p>

<pre><code>&gt; perl inflections.t

ok 1 - house -&gt; houses
ok 2 - law -&gt; laws
not ok 3 - mongoose -&gt; mongooses
#     Failed test (inflections.t at line 20)
#          got: 'mongeese'
#     expected: 'mongooses'
ok 4 - goose -&gt; geese
ok 5 - ox -&gt; oxen
not ok 6 - mother-in-law -&gt; mothers-in-law
#     Failed test (inflections.t at line 20)
#          got: 'mothers-in-laws'
#     expected: 'mothers-in-law'
ok 7 - mouse -&gt; mice
ok 8 - box -&gt; boxes
1..8
# Looks like you failed 2 tests of 8.</code></pre>

<p>The <a
href="http://search.cpan.org/perldoc?Test::Tutorial">Test::Tutorial</a>
documentation that comes with Perl 5.8 provides a gentle introduction to both
<code>Test::Simple</code> and <code>Test::More</code>.</p>

<h3>3. Create Standard POD Templates for Modules and Applications</h3>

<p>One of the main reasons documentation can often seem so unpleasant is the
"blank page effect." Many programmers simply don't know how to get started or
what to say.</p>

<p>Perhaps the easiest way to make writing documentation less forbidding (and
hence, more likely to actually occur) is to circumvent that initial empty screen
by providing a template that developers can cut and paste into their code.</p>

<p>For a module, that documentation template might look something like
this:</p>

<pre><code>=head1 NAME

&lt;Module::Name&gt; - &lt;One-line description of module's purpose&gt;

=head1 VERSION

The initial template usually just has:

This documentation refers to &lt;Module::Name&gt; version 0.0.1.

=head1 SYNOPSIS

   use &lt;Module::Name&gt;;

   # Brief but working code example(s) here showing the most common usage(s)
   # This section will be as far as many users bother reading, so make it as
   # educational and exemplary as possible.

=head1 DESCRIPTION

A full description of the module and its features.

May include numerous subsections (i.e., =head2, =head3, etc.).

=head1 SUBROUTINES/METHODS

A separate section listing the public components of the module's interface.

These normally consist of either subroutines that may be exported, or methods
that may be called on objects belonging to the classes that the module
provides.

Name the section accordingly.

In an object-oriented module, this section should begin with a sentence (of the
form "An object of this class represents ...") to give the reader a high-level
context to help them understand the methods that are subsequently described.

=head1 DIAGNOSTICS

A list of every error and warning message that the module can generate (even
the ones that will "never happen"), with a full explanation of each problem,
one or more likely causes, and any suggested remedies.

=head1 CONFIGURATION AND ENVIRONMENT

A full explanation of any configuration system(s) used by the module, including
the names and locations of any configuration files, and the meaning of any
environment variables or properties that can be set. These descriptions must
also include details of any configuration language used.

=head1 DEPENDENCIES

A list of all of the other modules that this module relies upon, including any
restrictions on versions, and an indication of whether these required modules
are part of the standard Perl distribution, part of the module's distribution,
or must be installed separately.

=head1 INCOMPATIBILITIES

A list of any modules that this module cannot be used in conjunction with.
This may be due to name conflicts in the interface, or competition for system
or program resources, or due to internal limitations of Perl (for example, many
modules that use source code filters are mutually incompatible).

=head1 BUGS AND LIMITATIONS

A list of known problems with the module, together with some indication of
whether they are likely to be fixed in an upcoming release.

Also, a list of restrictions on the features the module does provide: data types
that cannot be handled, performance issues and the circumstances in which they
may arise, practical limitations on the size of data sets, special cases that
are not (yet) handled, etc.

The initial template usually just has:

There are no known bugs in this module.

Please report problems to &lt;Maintainer name(s)&gt; (&lt;contact address&gt;)

Patches are welcome.

=head1 AUTHOR

&lt;Author name(s)&gt;  (&lt;contact address&gt;)

=head1 LICENSE AND COPYRIGHT

Copyright (c) &lt;year&gt; &lt;copyright holder&gt; (&lt;contact address&gt;).
All rights reserved.

followed by whatever license you wish to release it under.

For Perl code that is often just:

This module is free software; you can redistribute it and/or modify it under
the same terms as Perl itself. See L&lt;perlartistic&gt;.  This program is
distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.</code></pre>













<p>Of course, the specific details that your templates provide may vary from
those shown here, according to your other coding practices. The most likely
variation will be in the license and copyright, but you may also have specific
in-house conventions regarding version numbering, the grammar of diagnostic
messages, or the attribution of authorship.</p>

<h3>4. Use a Revision Control System</h3>

<p>Maintaining control over the creation and modification of your source code
is utterly essential for robust team-based development.  And not just over
source code: you should be revision controlling your documentation, and data
files, and document templates, and makefiles, and style sheets, and change
logs, and any other resources your system requires.</p>

<p>Just as you wouldn't use an editor without an Undo command or a word
processor that can't merge documents, so too you shouldn't use a file system
you can't rewind, or a development environment that can't integrate the work
of many contributors.</p>

<p>Programmers make mistakes, and occasionally those mistakes will be
catastrophic. They will reformat the disk containing the most recent version of the
code. Or they'll mistype an editor macro and write zeros all through the source
of a critical core module. Or two developers will unwittingly edit the same
file at the same time and half their changes will be lost. Revision control
systems can prevent those kinds of problems.</p>

<p>Moreover, occasionally the very best debugging technique is to just give up,
stop trying to get yesterday's modifications to work correctly, roll the code
back to a known stable state, and start over again. Less drastically, comparing
the current condition of your code with the most recent stable version from
your repository (even just a line-by-line <code>diff</code>) can often help you isolate your
recent "improvements" and work out which of them is the problem.</p>

<p>Revision control systems such as RCS, CVS, Subversion, Monotone, <code>darcs</code>,
Perforce, GNU arch, or BitKeeper can protect against calamities, and ensure
that you always have a working fallback position if maintenance goes horribly
wrong. The various systems have different strengths and limitations, many of
which stem from fundamentally different views on what exactly revision control
is. It's a good idea to audition the various revision control systems, and find
the one that works best for you.  <em>Pragmatic Version Control Using
Subversion</em>, by Mike Mason (Pragmatic Bookshelf, 2005) and <a href="http://www.oreilly.com/catalog/cvs/"><em>Essential
CVS</em></a>, by Jennifer Vesperman (O'Reilly, 2003) are useful starting
points.</p>

<h3>5. Create Consistent Command-Line Interfaces</h3>

<p>Command-line interfaces have a strong tendency to grow over time, accreting
new options as you add features to the application. Unfortunately, the
evolution of such interfaces is rarely designed, managed, or controlled, so the
set of flags, options, and arguments that a given application accepts are
likely to be ad hoc and unique.</p>

<p>This also means they're likely to be inconsistent with the unique ad hoc
sets of flags, options, and arguments that other related applications provide.
The result is inevitably a suite of programs, each of which is driven in a
distinct and idiosyncratic way. For example:</p>

<pre><code>&gt; orchestrate source.txt -to interim.orc

&gt; remonstrate +interim.rem -interim.orc 

&gt; fenestrate  --src=interim.rem --dest=final.wdw
Invalid input format

&gt; fenestrate --help
Unknown option: --help.
Type 'fenestrate -hmo' for help</code></pre>

<p>Here, the <code>orchestrate</code> utility expects its input file as its
first argument, while the <code>-to</code> flag specifies its output file. The
related <code>remonstrate</code> tool uses <code>-infile</code> and
<code>+outfile</code> options instead, with the output file coming first. The
<code>fenestrate</code> program seems to require GNU-style "long options:"
<code>--src=infile</code> and <code>--dest=outfile</code>, except, apparently,
for its oddly named help flag. All in all, it's a mess.</p>

<p>When you're providing a suite of programs, all of them should appear to
work the same way, using the same flags and options for the same features
across all applications. This enables your users to take advantage of existing
knowledge--instead of continually asking you.</p>

<p>Those three programs should work like this:</p>

<pre><code>&gt; orchestrate -i source.txt -o dest.orc

&gt; remonstrate -i source.orc -o dest.rem

&gt; fenestrate  -i source.rem -o dest.wdw
Input file ('source.rem') not a valid Remora file
(type "fenestrate --help" for help)

&gt; fenestrate --help
fenestrate - convert Remora .rem files to Windows .wdw format
Usage: fenestrate [-i &lt;infile&gt;] [-o &lt;outfile&gt;] [-cstq] [-h|-v]
Options:
   -i &lt;infile&gt; Specify input source [default: STDIN]
   -o &lt;outfile&gt; Specify output destination [default: STDOUT]
   -c Attempt to produce a more compact representation
   -h Use horizontal (landscape) layout
   -v Use vertical (portrait) layout
   -s Be strict regarding input
   -t Be extra tolerant regarding input
   -q Run silent
   --version Print version information
   --usage Print the usage line of this summary
   --help Print this summary
   --man Print the complete manpage</code></pre>

<p>Here, every application that takes input and output files uses the same two
flags to do so. A user who wants to use the <code>substrate</code> utility (to convert that
final .wdw file to a subroutine) is likely to be able to guess
correctly the required syntax:</p>

<pre><code>&gt; substrate  -i dest.wdw -o dest.sub</code></pre>

<p>Anyone who can't guess that probably can guess that:</p>

<pre><code>&gt; substrate --help</code></pre>

<p>is likely to render aid and comfort.</p>













<p>A large part of making interfaces consistent is being consistent in
specifying the individual components of those interfaces. Some conventions that
may help to design consistent and predictable interfaces include:</p>

<ul>

<li><p>Require a flag preceding every piece of command-line data, except
filenames.</p>

<p>Users don't want to have to remember that your application requires "input
file, output file, block size, operation, fallback strategy," and requires
them in that precise order:</p>

<pre><code>&gt; lustrate sample_data proc_data 1000 normalize log</code></pre>

<p>They want to be able to say explicitly what they mean, in any order that
suits them:</p>

<pre><code>&gt; lustrate sample_data proc_data -op=normalize -b1000 --fallback=log</code></pre></li>

<li><p>Provide a flag for each filename, too, especially when a program can be
given files for different purposes.</p>

<p>Users might also not want to remember the order of the two positional
filenames, so let them label those arguments as well, and specify them in
whatever order they prefer:</p>

<pre><code>&gt; lustrate -i sample_data -op normalize -b1000 --fallback log -o proc_data</code></pre></li>

<li><p>Use a single <code>-</code> prefix for short-form flags, up to three
letters (<code>-v</code>, <code>-i</code>, <code>-rw</code>, <code>-in</code>,
<code>-out</code>).</p>

<p>Experienced users appreciate short-form flags as a way of reducing typing
and limiting command-line clutter. Don't make them type two dashes in these
shortcuts.</p></li>

<li><p>Use a double <code>--</code> prefix for longer flags
(<code>--verbose</code>, <code>--interactive</code>, <code>--readwrite</code>,
<code>--input</code>, <code>--output</code>).</p>

<p>Flags that are complete words improve the readability of a command line (in
a shell script, for example). The double dash also helps to distinguish between
the longer flag name and any nearby file names.</p></li>

<li><p>If a flag expects an associated value, allow an optional <code>=</code> between the
flag and the value.</p>

<p>Some people prefer to visually associate a value with its preceding
flag:</p>

<pre><code>&gt; lustrate -i=sample_data -op=normalize -b=1000 --fallback=log -o=proc_data</code></pre>

<p>Others don't:</p>

<pre><code>&gt; lustrate -i sample_data -op normalize -b1000 --fallback log -o proc_data</code></pre>

<p>Still others want a bit each way:</p>

<pre><code>&gt; lustrate -i sample_data -o proc_data -op=normalize -b=1000 --fallback=log</code></pre>

<p>Let the user choose.</p></li>

<li><p>Allow single-letter options to be "bundled" after a single dash.</p>

<p>It's irritating to have to type repeated dashes for a series of flags:</p>

<pre><code>&gt; lustrate -i sample_data -v -l -x</code></pre>

<p>Allow experienced users to also write:</p>

<pre><code>&gt; lustrate -i sample_data -vlx</code></pre></li>

<li><p>Provide a multi-letter version of every single-letter flag.</p>

<p>Short-form flags may be nice for experienced users, but they can be
troublesome for new users: hard to remember and even harder to recognize. Don't
force people to do either. Give them a verbose alternative to every concise
flag; full words that are easier to remember, and also more self-documenting in
shell scripts.</p></li>

<li><p>Always allow <code>-</code> as a special filename.</p>

<p>A widely used convention is that a dash (<code>-</code>) where an input file
is expected means "read from standard input," and a dash where an output file
is expected means "write to standard output."</p></li>

<li><p>Always allow <code>--</code> as a file list marker.</p>

<p>Another widely used convention is that the appearance of a double dash
(<code>--</code>) on the command line marks the end of any flagged options, and
indicates that the remaining arguments are a list of filenames, even if some of
them look like flags.</p></li>

</ul>

<h3>6. Agree Upon a Coherent Layout Style and Automate It with <code>perltidy</code></h3>

<p>Formatting. Indentation. Style. Code layout. Whatever you choose to call it,
it's one of the most contentious aspects of programming discipline. More and
bloodier wars have been fought over code layout than over just about any other
aspect of coding.</p>

<p>What is the best practice here? Should you use classic Kernighan and
Ritchie style? Or go with BSD code formatting? Or adopt the layout scheme
specified by the GNU project? Or conform to the Slashcode coding
guidelines?</p>

<p>Of course not! Everyone knows that <em>&lt;insert your personal coding style here&gt;</em>
is the One True Layout Style, the only sane choice, as ordained by <em>&lt;insert your
favorite Programming Deity here&gt;</em> since Time Immemorial! Any other choice is
manifestly absurd, willfully heretical, and self-evidently a Work of
Darkness!</p>

<p>That's precisely the problem. When deciding on a layout style, it's hard to
decide where rational choices end and rationalized habits begin.</p>

<p>Adopting a coherently designed approach to code layout, and then applying
that approach consistently across all your coding, is fundamental to best-practice programming. Good layout can improve the readability of a program,
help detect errors within it, and make the structure of your code much easier
to comprehend. Layout matters.</p>

<p>However, most coding styles--including the four mentioned earlier--confer
those benefits almost equally well. While it's true that having a consistent
code layout scheme matters very much indeed, the particular code layout scheme
you ultimately decide upon does not matter at all!  All that matters is that
you adopt a single, coherent style; one that works for your entire programming
team, and, having agreed upon that style, that you then apply it consistently
across all your development.</p>













<p>In the long term, it's best to train yourself and your team to code in a
consistent, rational, and readable style. However, the time and commitment
necessary to accomplish that isn't always available. In such cases, a
reasonable compromise is to prescribe a standard code-formatting tool that must
be applied to all code before it's committed, reviewed, or otherwise displayed
in public.</p>

<p>There is now an excellent code formatter available for Perl: <a
href="http://perltidy.sourceforge.net/"><code>perltidy</code></a>. It provides an extensive
range of user-configurable options for indenting, block delimiter positioning,
column-like alignment, and comment positioning.</p>

<p>Using <code>perltidy</code>, you can convert code like this:</p>

<pre><code>if($sigil eq '$'){
   if($subsigil eq '?'){ 
       $sym_table{substr($var_name,2)}=delete $sym_table{locate_orig_var($var)};
       $internal_count++;$has_internal{$var_name}++
   } else {
       ${$var_ref} =
           q{$sym_table{$var_name}}; $external_count++; $has_external{$var_name}++;
}} elsif ($sigil eq '@'&amp;&amp;$subsigil eq '?') {
   @{$sym_table{$var_name}} = grep
       {defined $_} @{$sym_table{$var_name}};
} elsif ($sigil eq '%' &amp;&amp; $subsigil eq '?') {
delete $sym_table{$var_name}{$EMPTY_STR}; } else
{
${$var_ref}
=
q{$sym_table{$var_name}}
}</code></pre>

<p>into something readable:</p>

<pre><code>if ( $sigil eq '$' ) {
   if ( $subsigil eq '?' ) {
       $sym_table{ substr( $var_name, 2 ) }
           = delete $sym_table{ locate_orig_var($var) };
       $internal_count++;
       $has_internal{$var_name}++;
   }
   else {
       ${$var_ref} = q{$sym_table{$var_name}};
       $external_count++;
       $has_external{$var_name}++;
   }
}
elsif ( $sigil eq '@' &amp;&amp; $subsigil eq '?' ) {
   @{ $sym_table{$var_name} }
       = grep {defined $_} @{ $sym_table{$var_name} };
}
elsif ( $sigil eq '%' &amp;&amp; $subsigil eq '?' ) {
   delete $sym_table{$var_name}{$EMPTY_STR};
}
else {
   ${$var_ref} = q{$sym_table{$var_name}};
}</code></pre>

<p>Mandating that everyone use a common tool to format their code can also be a
simple way of sidestepping the endless objections, acrimony, and dogma that
always surround any discussion on code layout. If <code>perltidy</code> does all the work
for them, then it will cost developers almost no effort to adopt the new
guidelines. They can simply set up an editor macro that will "straighten" their
code whenever they need to.</p>

<h3>7. Code in Commented Paragraphs</h3>

<p>A paragraph is a collection of statements that accomplish a single task: in
literature, it's a series of sentences conveying a single idea; in programming,
a series of instructions implementing a single step of an algorithm.</p>

<p>Break each piece of code into sequences that achieve a single task, placing
a single empty line between each sequence. To further improve the
maintainability of the code, place a one-line comment at the start of each such
paragraph, describing what the sequence of statements does. Like so:</p>

<pre><code># Process an array that has been recognized...
sub addarray_internal {
   my ($var_name, $needs_quotemeta) = @_;

   # Cache the original...
   $raw .= $var_name;

   # Build meta-quoting code, if requested...
   my $quotemeta = $needs_quotemeta ?  q{map {quotemeta $_} } : $EMPTY_STR;

   # Expand elements of variable, conjoin with ORs...
   my $perl5pat = qq{(??{join q{|}, $quotemeta \@{$var_name}})};

   # Insert debugging code if requested...
   my $type = $quotemeta ? 'literal' : 'pattern';
   debug_now("Adding $var_name (as $type)");
   add_debug_mesg("Trying $var_name (as $type)");

   return $perl5pat;
}</code></pre>

<p>Paragraphs are useful because humans can focus on only a few pieces of
information at once. Paragraphs are one way of aggregating small amounts of
related information, so that the resulting "chunk" can fit into a single slot
of the reader's limited short-term memory. Paragraphs enable the physical
structure of a piece of writing to reflect and emphasize its logical
structure.</p>

<p>Adding comments at the start of each paragraph further enhances the chunking
by explicitly summarizing the purpose of each chunk (note: the purpose, not the
behavior). Paragraph comments need to explain why the code is there and what it
achieves, not merely paraphrase the precise computational steps it's
performing.</p>













<p>Note, however, that the contents of paragraphs are only of secondary
importance here. It is the vertical gaps separating each paragraph that are
critical. Without them, the readability of the code declines dramatically, even
if the comments are retained:</p>

<pre><code>sub addarray_internal {
   my ($var_name, $needs_quotemeta) = @_;
   # Cache the original...
   $raw .= $var_name;
   # Build meta-quoting code, if required...
   my $quotemeta = $needs_quotemeta ?  q{map {quotemeta $_} } : $EMPTY_STR;
   # Expand elements of variable, conjoin with ORs...
   my $perl5pat = qq{(??{join q{|}, $quotemeta \@{$var_name}})};
   # Insert debugging code if requested...
   my $type = $quotemeta ? 'literal' : 'pattern';
   debug_now("Adding $var_name (as $type)");
   add_debug_mesg("Trying $var_name (as $type)");
   return $perl5pat;
}</code></pre>

<h3>8. Throw Exceptions Instead of Returning Special Values or Setting
Flags</h3>

<p>Returning a special error value on failure, or setting a special error flag,
is a very common error-handling technique.  Collectively, they're the basis for
virtually all error notification from Perl's own built-in functions. For
example, the built-ins <code>eval</code>, <code>exec</code>, <code>flock</code>,
<code>open</code>, <code>print</code>, <code>stat</code>, and
<code>system</code> all return special values on error. Unfortunately, they
don't all use the same special value. Some of them also set a flag on failure.
Sadly, it's not always the same flag. See the <a
href="http://perldoc.perl.org/perlfunc.html"><code>perlfunc</code></a> manpage for the gory
details.</p>

<p>Apart from the obvious consistency problems, error notification via flags
and return values has another serious flaw: developers can silently ignore
flags and return values, and ignoring them requires absolutely no effort on the
part of the programmer. In fact, in a void context, ignoring return values is
Perl's default behavior. Ignoring an error flag that has suddenly appeared in
a special variable is just as easy: you simply don't bother to check the
variable.</p>

<p>Moreover, because ignoring a return value is the void-context default,
there's no syntactic marker for it. There's no way to look at a program and
immediately see where a return value is deliberately being ignored, which means
there's also no way to be sure that it's not being ignored accidentally.</p>

<p>The bottom line: regardless of the programmer's (lack of) intention, an
error indicator is being ignored. That's not good programming.</p>

<p>Ignoring error indicators frequently causes programs to propagate errors in
entirely the wrong direction. For example:</p>

<pre><code># Find and open a file by name, returning the filehandle
# or undef on failure...
sub locate_and_open {
   my ($filename) = @_;

   # Check acceptable directories in order...
   for my $dir (@DATA_DIRS) {
       my $path = "$dir/$filename";

       # If file exists in an acceptable directory, open and return it...
       if (-r $path) {
           open my $fh, '&lt;', $path;
           return $fh;
       }
   }

   # Fail if all possible locations tried without success...
   return;
}

# Load file contents up to the first &lt;DATA/&gt; marker...
sub load_header_from {
   my ($fh) = @_;

   # Use DATA tag as end-of-"line"...
   local $/ = '&lt;DATA/&gt;';

   # Read to end-of-"line"...
   return &lt;$fh&gt;;
}

# and later...
for my $filename (@source_files) {
   my $fh = locate_and_open($filename);
   my $head = load_header_from($fh);
   print $head;
}</code></pre>

<p>The <code>locate_and_open()</code> subroutine simply assumes that the call
to <code>open</code> works, immediately returning the filehandle
(<code>$fh</code>), whatever the actual outcome of the <code>open</code>. Presumably, the
expectation is that whoever calls <code>locate_and_open()</code> will check
whether the return value is a valid filehandle.</p>

<p>Except, of course, "whoever" doesn't check. Instead of testing for failure,
the main <code>for</code> loop takes the failure value and immediately
propagates it "across" the block, to the rest of the statements in the loop.
That causes the call to <code>loader_header_from()</code> to propagate the
error value "downwards." It's in that subroutine that the attempt to treat the
failure value as a filehandle eventually kills the program:</p>

<pre><code>readline() on unopened filehandle at demo.pl line 28.</code></pre>

<p>Code like that--where an error is reported in an entirely different part of
the program from where it actually occurred--is particularly onerous to
debug.</p>

<p>Of course, you could argue that the fault lies squarely with whoever wrote
the loop, for using <code>locate_and_open()</code> without checking its return
value.  In the narrowest sense, that's entirely correct--but the deeper fault
lies with whoever actually wrote <code>locate_and_open()</code> in the first
place, or at least, whoever assumed that the caller would always check its
return value.</p>













<p>Humans simply aren't like that. Rocks almost never fall out of the sky, so
humans soon conclude that they never do, and stop looking up for them.  Fires
rarely break out in their homes, so humans soon forget that they might, and
stop testing their smoke detectors every month. In the same way, programmers
inevitably abbreviate "almost never fails" to "never fails," and then simply
stop checking.</p>

<p>That's why so very few people bother to verify their <code>print</code>
statements:</p>

<pre><code>if (!print 'Enter your name: ') {
   print {*STDLOG} warning =&gt; 'Terminal went missing!'
}</code></pre>

<p>It's human nature to "trust but not verify."</p>

<p>Human nature is why returning an error indicator is not best practice.
Errors are (supposed to be) unusual occurrences, so error markers will almost
never be returned. Those tedious and ungainly checks for them will almost never
do anything useful, so eventually they'll be quietly omitted. After all,
leaving the tests off almost always works just fine. It's so much easier not to
bother. Especially when not bothering is the default!</p>

<p>Don't return special error values when something goes wrong; throw an
exception instead. The great advantage of exceptions is that they reverse the
usual default behaviors, bringing untrapped errors to immediate and urgent
attention. On the other hand, ignoring an exception requires a deliberate and
conspicuous effort: you have to provide an explicit <code>eval</code> block to
neutralize it.</p>

<p>The <code>locate_and_open()</code> subroutine would be much cleaner and more
robust if the errors within it threw exceptions:</p>

<pre><code># Find and open a file by name, returning the filehandle
# or throwing an exception on failure...
sub locate_and_open {
   my ($filename) = @_;

   # Check acceptable directories in order...
   for my $dir (@DATA_DIRS) {
       my $path = "$dir/$filename";

       # If file exists in acceptable directory, open and return it...
       if (-r $path) {
           open my $fh, '&lt;', $path
               or croak( "Located $filename at $path, but could not open");
           return $fh;
       }
   }

   # Fail if all possible locations tried without success...
   croak( "Could not locate $filename" );
}

# and later...
for my $filename (@source_files) {
   my $fh = locate_and_open($filename);
   my $head = load_header_from($fh);
   print $head;
}</code></pre>

<p>Notice that the main <code>for</code> loop didn't change at all. The
developer using <code>locate_and_open()</code> still assumes that nothing can
go wrong. Now there's some justification for that expectation, because if
anything does go wrong, the thrown exception will automatically terminate the
loop.</p>

<p>Exceptions are a better choice even if you are the careful type who
religiously checks every return value for failure:</p>

<pre><code>SOURCE_FILE:
for my $filename (@source_files) {
   my $fh = locate_and_open($filename);
   next SOURCE_FILE if !defined $fh;
   my $head = load_header_from($fh);
   next SOURCE_FILE if !defined $head;
   print $head;
}</code></pre>

<p>Constantly checking return values for failure clutters your code with
validation statements, often greatly decreasing its readability. In contrast,
exceptions allow an algorithm to be implemented without having to intersperse
any error-handling infrastructure at all. You can factor the error-handling out
of the code and either relegate it to after the surrounding <code>eval</code>,
or else dispense with it entirely:</p>

<pre><code>for my $filename (@directory_path) {

   # Just ignore any source files that don't load...
   eval {
       my $fh = locate_and_open($filename);
       my $head = load_header_from($fh);
       print $head;
   }
}</code></pre>

<h3>9. Add New Test Cases Before you Start Debugging</h3>

<p>The first step in any debugging process is to isolate the incorrect
behavior of the system, by producing the shortest demonstration of it that you
reasonably can. If you're lucky, this may even have been done for you:</p>

<pre><code>To: DCONWAY@cpan.org
From: sascha@perlmonks.org
Subject: Bug in inflect module

Zdravstvuite,

I have been using your Lingua::EN::Inflect module to normalize terms in a
data-mining application I am developing, but there seems to be a bug in it,
as the following example demonstrates:

   use Lingua::EN::Inflect qw( PL_N );
   print PL_N('man'), "\n";       # Prints "men", as expected
   print PL_N('woman'), "\n";     # Incorrectly prints "womans"</code></pre>

<p>Once you have distilled a short working example of the bug, convert it to a
series of tests, such as:</p>

<pre><code>use Lingua::EN::Inflect qw( PL_N );
use Test::More qw( no_plan );
is(PL_N('man') ,  'men', 'man -&gt; men'     );
is(PL_N('woman'), 'women', 'woman -&gt; women' );</code></pre>













<p>Don't try to fix the problem straight away, though. Instead, immediately add
those tests to your test suite. If that testing has been well set up, that can
often be as simple as adding a couple of entries to a table:</p>

<pre><code>my %plural_of = (
   'mouse'         =&gt; 'mice',
   'house'         =&gt; 'houses',
   'ox'            =&gt; 'oxen',
   'box'           =&gt; 'boxes',
   'goose'         =&gt; 'geese',
   'mongoose'      =&gt; 'mongooses', 
   'law'           =&gt; 'laws',
   'mother-in-law' =&gt; 'mothers-in-law', 

   # Sascha's bug, reported 27 August 2004...
   'man'           =&gt; 'men',
   'woman'         =&gt; 'women',
);</code></pre>

<p>The point is: if the original test suite didn't report this bug, then that
test suite was broken. It simply didn't do its job (finding bugs) adequately.
Fix the test suite first by adding tests that cause it to fail:</p>

<pre><code>&gt; perl inflections.t
ok 1 - house -&gt; houses
ok 2 - law -&gt; laws
ok 3 - man -&gt; men
ok 4 - mongoose -&gt; mongooses
ok 5 - goose -&gt; geese
ok 6 - ox -&gt; oxen
not ok 7 - woman -&gt; women
#     Failed test (inflections.t at line 20)
#          got: 'womans'
#     expected: 'women'
ok 8 - mother-in-law -&gt; mothers-in-law
ok 9 - mouse -&gt; mice
ok 10 - box -&gt; boxes
1..10
# Looks like you failed 1 tests of 10.</code></pre>

<p>Once the test suite is detecting the problem correctly, then you'll be able
to tell when you've correctly fixed the actual bug, because the tests will once
again fall silent.</p>

<p>This approach to debugging is most effective when the test suite covers the
full range of manifestations of the problem. When adding test cases for a bug,
don't just add a single test for the simplest case. Make sure you include the
obvious variations as well:</p>

<pre><code>my %plural_of = (
   'mouse'         =&gt; 'mice',
   'house'         =&gt; 'houses',
   'ox'            =&gt; 'oxen',
   'box'           =&gt; 'boxes',
   'goose'         =&gt; 'geese',
   'mongoose'      =&gt; 'mongooses', 
   'law'           =&gt; 'laws',
   'mother-in-law' =&gt; 'mothers-in-law', 

   # Sascha's bug, reported 27 August 2004...
   'man'           =&gt; 'men',
   'woman'         =&gt; 'women',
   'human'         =&gt; 'humans',
   'man-at-arms'   =&gt; 'men-at-arms', 
   'lan'           =&gt; 'lans',
   'mane'          =&gt; 'manes',
   'moan'          =&gt; 'moans',
);</code></pre>

<p>The more thoroughly you test the bug, the more completely you will fix
it.</p>

<h3>10. Don't Optimize Code--Benchmark It</h3>

<p>If you need a function to remove duplicate elements of an array, it's
natural to think that a "one-liner" like this:</p>

<pre><code>sub uniq { return keys %{ { map {$_=&gt;1} @_ } } }</code></pre>

<p>will be more efficient than two statements:</p>

<pre><code>sub uniq {
   my %seen;
   return grep {!$seen{$_}++} @_;
}</code></pre>

<p>Unless you are deeply familiar with the internals of the Perl interpreter
(in which case you already have far more serious personal issues to deal with),
intuitions about the relative performance of two constructs are exactly that:
unconscious guesses.</p>

<p>The only way to know for sure which of two--or more--alternatives will perform
better is to actually time each of them. The standard <a
href="http://search.cpan.org/perldoc?Benchmark">Benchmark</a> module makes that
easy:</p>

<pre><code># A short list of not-quite-unique values...
our @data = qw( do re me fa so la ti do );

# Various candidates...
sub unique_via_anon {
   return keys %{ { map {$_=&gt;1} @_ } };
}

sub unique_via_grep {
   my %seen;
   return grep { !$seen{$_}++ } @_;
}

sub unique_via_slice {
   my %uniq;
   @uniq{@_} = ();
   return keys %uniq;
}

# Compare the current set of data in @data
sub compare {
   my ($title) = @_;
   print "\n[$title]\n";

   # Create a comparison table of the various timings, making sure that
   # each test runs at least 10 CPU seconds...
   use Benchmark qw( cmpthese );
   cmpthese -10, {
       anon  =&gt; 'my @uniq = unique_via_anon(@data)',
       grep  =&gt; 'my @uniq = unique_via_grep(@data)',
       slice =&gt; 'my @uniq = unique_via_slice(@data)',
   };

   return;
}

compare('8 items, 10% repetition');

# Two copies of the original data...
@data = (@data) x 2;
compare('16 items, 56% repetition');

# One hundred copies of the original data...
@data = (@data) x 50;
compare('800 items, 99% repetition');</code></pre>

<p>The <code>cmpthese()</code> subroutine takes a number, followed by a
reference to a hash of tests. The number specifies either the exact number of
times to run each test (if the number is positive), or the absolute number of
CPU seconds to run the test for (if the number is negative). Typical values are
around 10,000 repetitions or ten CPU seconds, but the module will warn you if
the test is too short to produce an accurate benchmark.</p>

<p>The keys of the test hash are the names of your tests, and the corresponding
values specify the code to be tested. Those values can be either strings (which
are <code>eval</code>'d to produce executable code) or subroutine references
(which are called directly).</p>

<p>The benchmarking code shown above would print out something like the
following:</p>

<pre><code>[8 items, 10% repetitions]
        Rate anon  grep slice
anon  28234/s --  -24%  -47%
grep  37294/s   32% --  -30%
slice 53013/s   88% 42%    --

[16 items, 50% repetitions]
        Rate anon  grep slice
anon  21283/s --  -28%  -51%
grep  29500/s   39% --  -32%
slice 43535/s  105% 48%    --

[800 items, 99% repetitions]
       Rate  anon grep slice
anon   536/s --  -65%  -89%
grep  1516/s  183% --  -69%
slice 4855/s  806%  220% --</code></pre>

<p>Each of the tables printed has a separate row for each named test. The
first column lists the absolute speed of each candidate in repetitions per
second, while the remaining columns allow you to compare the relative
performance of any two tests. For example, in the final test tracing across
the <code>grep</code> row to the <code>anon</code> column reveals that the
<code>grep</code>ped solution was 1.83 times (183 percent) faster than using an anonymous hash.
Tracing further across the same row also indicates that <code>grep</code>ping was 69 percent
slower (-69 percent faster) than slicing.</p>

<p>Overall, the indication from the three tests is that the slicing-based
solution is consistently the fastest for this particular set of data on this
particular machine. It also appears that as the data set increases in size,
slicing also scales much better than either of the other two approaches.</p>

<p>However, those two conclusions are effectively drawn from only three data
points (namely, the three benchmarking runs). To get a more definitive
comparison of the three methods, you'd also need to test other possibilities,
such as a long list of non-repeating items, or a short list with nothing but
repetitions.</p>

<p>Better still, test on the real data that you'll actually be
"unique-ing."</p>

<p>For example, if that data is a sorted list of a quarter of a million words, with
only minimal repetitions, and which has to remain sorted, then test exactly
that:</p>

<pre><code>our @data = slurp '/usr/share/biglongwordlist.txt';

use Benchmark qw( cmpthese );

cmpthese 10, {
    # Note: the non-grepped solutions need a post-uniqification re-sort
    anon  =&gt; 'my @uniq = sort(unique_via_anon(@data))',
    grep  =&gt; 'my @uniq = unique_via_grep(@data)',
    slice =&gt; 'my @uniq = sort(unique_via_slice(@data))',
};</code></pre>

<p>Not surprisingly, this benchmark indicates that the <code>grep</code>ped solution is
markedly superior on a large sorted data set:</p>

<pre><code>s/iter anon slice  grep
anon    4.28 --   -3%  -46%
slice   4.15 3%    --  -44%
grep    2.30 86%   80%    --</code></pre>

<p>Perhaps more interestingly, the <code>grep</code>ped solution still benchmarks as being
marginally faster when the two hash-based approaches aren't re-sorted. This
suggests that the better scalability of the sliced solution as seen in the
earlier benchmark is a localized phenomenon, and is eventually undermined by
the growing costs of allocation, hashing, and bucket-overflows as the sliced
hash grows very large.</p>

<p>Above all, that last example demonstrates that benchmarks only benchmark the
cases you actually benchmark, and that you can only draw useful conclusions
about performance from benchmarking real data.</p>




        </div>



    </div>
    <div class="asset-footer"></div>
</div>


                            
                            <div id="entry-748" class="entry-asset asset hentry">
    <div class="asset-header">
        <h2 class="asset-name entry-title"><a href="/pub/2005/04/mockobject-kata.html" rel="bookmark">Perl Code Kata: Mocking Objects</a></h2>
        <div class="asset-meta">
            <span class="byline">
    
                By <span class="vcard author">Stevan Little</span> on <abbr class="published" title="2005-04-07T00:00:00-08:00">April  7, 2005 12:00 AM</abbr>
    
            </span>

            
            

        </div>
    </div>
    <div class="asset-content entry-content">

        <div class="asset-body">
            
<!-- sidebar begins -->
<!-- don't move sidebars-->
<!-- sidebar ends -->
<p>The last Perl Code Kata was on <a
href="http://search.cpan.org/perldoc?DBD::Mock">DBD::Mock</a>, a mock DBI
driver which is useful for <a
href="/pub/a/2005/02/10/database_kata.html">testing Perl DBI
applications</a>. This Kata delves once again into the world of mock objects,
this time using the more general <a
href="http://search.cpan.org/perldoc?Test::MockObject">Test::MockObject</a>
module.</p>

<h3>What are Mock Objects?</h3>

<p>Mock objects are exactly what they sound like: "mocked" or "fake"
objects. Through the power of polymorphism, it's easy to swap one object for
another object which implements the same interface. Mock objects take advantage
of this fact, allowing you to substitute the <em>most minimally mocked
implementation of an object possible</em> for the real one during testing. This
allows a greater degree of isolation within your tests, which is just an all
around good thing.</p>

<h3>What are Mock Objects Good For?</h3>

<p>Mock objects are primarily useful when writing unit tests. They share a
certain similarity with the Null Object pattern in that they are
purposefully <em>not</em> meant to work. Mock objects take things one step
further and allow you to mock certain actions or reactions that your mock
object should have, so they are especially useful in scenarios usually
considered <em>hard to test</em>. Here is a short list of some scenarios in
which mock objects make hard things easy.</p>

<ul>

<li><p><em>Tests which depend on outside resources such as networks, databases,
etc.</em></p>

<p>If your code properly encapsulates any outside resources, then it should be
possible to substitute a mocked object in its place during testing.  This is
especially useful when you have little control over the execution environment
of your module. The previous Test Code Kata illustrated this by mocking the
database itself.  You need not stop with databases; you can mock any
sufficiently encapsulated resource such as network connections, files, or
miscellaneous external devices.</p></li>

<!-- sidebar begins -->
 <csperl file="grab" domain="on" record="b/1424" template="b/article_sidebar.view">
<!-- sidebar ends -->

<li><p><em>Tests for which dependencies require a lot of setup.</em></p>

<p>Sometimes your object will have a dependency which requires a large amount
of set-up code. The more non-test code in your tests, the higher the
possibility that it will contain a bug which can then corrupt your test
results. Many times your code uses only a small portion of this hard-to-setup
dependency as well. Mock objects can help simplify things by allowing you to
create the most minimally mocked implementation of an object and its
dependencies possible, thus removing the burden of the set-up code and reducing
the possibility of bugs in your non-test code.</p></li>

<li><p><em>Tests for failures; in particular, failure edge cases.</em></p>

<p>Testing for failures can sometimes be very difficult to do, especially
when the failure is not immediate, but triggered by a more subtle set of
interactions. Using mock objects, it is possible to achieve exacting
control over when, where, and why your object will fail. Mock
objects often make this kind of testing trivial.</p></li>

<li><p><em>Tests with optional dependencies.</em></p>

<p>Good code should be flexible code. Many times this means that your code
needs to adapt to many different situations and many different environments
based on the resources available at runtime. Requiring the presence of these
situations and/or environments in order to test your code can be very difficult
to set up or to tear down.  Just as with testing failures, it is possible to
use mock objects to achieve a high degree of control over your environment and
mock the situations you need to test.</p></li>

</ul>

<h3>The Problem</h3>

<p>The example code for this kata illustrates as many points as possible about
which mock objects are good at testing. Here is the code:</p>

<pre><code>package Site::Member;

use strict;
our $VERSION = '0.01';

sub new { bless { ip_address =&gt; '' }, shift }

sub ip_address { 
    my ($self, $ip_address) = @_;
    $self-&gt;{ip_address} = $ip_address if $ip_address;
    return $self-&gt;{ip_address};
}

# ...

sub city {
    my ($self) = @_;
    eval "use Geo::IP";
    if ($@) {
        warn "You must have Geo::IP installed for this feature";
        return;
    }
    my $geo = Geo::IP-&gt;open(
                "/usr/local/share/GeoIP/GeoIPCity.dat", 
                Geo::IP-&gt;GEOIP_STANDARD
            ) || die "Could not create a Geo::IP object with City data";
    my $record = $geo-&gt;record_by_addr($self-&gt;ip_address());
    return $record-&gt;city();
}</code></pre>

<p>This example code comes from a fictional online community software package.
Many such sites offer user homepages which can display all sorts of user
information. As an optional feature, the software can use the member's IP
address along with the <a
href="http://search.cpan.org/perldoc?Geo::IP">Geo::IP</a> module to determine
the user's city. The reason this feature is optional is that while
<code>Geo::IP</code> and the C library it uses are both free, the city data is
not.</p>

<p>The use cases suggest testing for the following scenarios:</p>

<ul>

<li>User does not have <code>Geo::IP</code> installed.</li>

<li>User has <code>Geo::IP</code> installed but does not have the city
data.</li>

<li>User has <code>Geo::IP</code> and city data installed correctly.</li>

</ul>

<p>Using <code>Test::MockObject</code>, take thirty to forty minutes and see if
you can write tests which cover all these use cases.</p>

<h3>Tips, Tricks, and Suggestions</h3>

<p>Some of the real strengths of <code>Test::MockObject</code> lie in its
adaptability and how simply it adapts. All
<code>Test::MockObject</code> sessions begin with creating an instance.</p>

<pre><code>my $mock = Test::MockObject-&gt;new();</code></pre>

<p>Even just this much can be useful because a <code>Test::MockObject</code>
instance warns about all un-mocked methods called on it. I have used this
"feature" to help trace calls while writing complex tests.</p>

<p>The next step is to mock some methods. The simplest approach is to use the
<code>mock</code> method. It takes a method name and a subroutine reference.
Every time something calls that method on the object, your <code>$mock</code>
instance will run that sub.</p>

<pre><code>$mock-&gt;mock('greetings' =&gt; sub {
    my ($mock, $name) = @_;
    return "Hello $name";
});</code></pre>

<p>How much simpler could it be?</p>

<p><code>Test::MockObject</code> also offers several pre-built mock method
builders, such as <code>set_true</code>, <code>set_false</code>, and
<code>set_always</code>. These methods pretty much DWIM.</p>

<pre><code>$mock-&gt;set_true('foo'); # the foo() method will return true
$mock-&gt;set_false('bar'); # the bar() method will return false
$mock-&gt;set_always('baz' =&gt; 100); # the bar() method will always return 100

</code></pre>

<p>It's even possible for the object to mock not only the methods, but its class as well. The simplest approach is to use the <code>set_isa</code>
method to tell the <code>$mock</code> object to pretend that it belongs to
another class.</p>

<pre><code>$mock-&gt;set_isa('Foo::Bar');</code></pre>

<p>Now, any code that calls this mock object's <code>isa()</code> method will
believe that the <code>$mock</code> is a <code>Foo::Bar</code> object.</p>

<p>In many cases, it is enough to substitute a <code>$mock</code> instance for
a real one and let polymorphism do the rest. Other times it is necessary to
inject control into the code much earlier than this. This is where the
<code>fake_module</code> method comes in.</p>

<p>With the <code>fake_module</code> method, <code>Test::MockObject</code> can
subvert control of an entire package such that it will intercept any calls to
that package. The following code:</p>

<pre><code>my $mock = Test::MockObject-&gt;new();
$mock-&gt;fake_module('Foo::Bar' =&gt; (
    'import' =&gt; sub { die "Foo::Bar could not be loaded" }
));
use_ok('Foo::Bar');</code></pre>

<p>...actually gives the illusion that the <code>Foo::Bar</code> module failed to
load regardless of whether the user has it installed. These kinds of edge cases
can be very difficult to test, but <code>Test::MockObject</code> simplifies
them greatly.</p>

<p>But wait, that's not all.</p>

<p>After your tests have run using your mock objects, it is possible to inspect
the methods called on them and query the order of their calls.  You can even
inspect the arguments passed into these methods.  There several methods for
this, so I refer you to the POD documentation of <code>Test::MockObject</code>
for details.</p>













<h3>The Solution</h3>

<p>I designed each use case to illustrate a different capability of
<code>Test::MockObject</code>.</p>

<ul>

<li><p>User does not have Geo::IP installed.</p>

<pre><code>use Test::More tests =&gt; 4;
use Test::MockObject;

my $mock = Test::MockObject-&gt;new();
$mock-&gt;fake_module('Geo::IP' =&gt; (
    'import' =&gt; sub { die "Could not load Geo::IP" },
));

use_ok('Site::Member');

my $u = Site::Member-&gt;new();
isa_ok($u, 'Site::Member');

my $warning;
local $SIG{__WARN__} = sub { $warning = shift };

ok(!defined($u-&gt;city()), '... this should return undef');
like($warning, 
        qr/^You must have Geo\:\:IP installed for this feature/, 
        '... and we should have our warning');</code></pre>

<p>This use case illustrates the use of <code>Test::MockObject</code> to mock
the failure of the loading of an optional resource, which in this case is the
<code>Geo::IP</code> module.</p>

<p>The sample code attempts to load <code>Geo::IP</code> by calling <code>eval
"use Geo::IP"</code>. Because <code>use</code> always calls a module's

<code>import</code> method, it is possible to exploit this and mock a
<code>Geo::IP</code> load failure. This is easy to accomplish by using the
<code>fake_module</code> method and making the <code>import</code> method
die. This then triggers the warning code in the <code>city</code> method, which
the <code>$SIG{__WARN__}</code> handler captures into <code>$warning</code> for
a later test.</p>

<p>This is an example of a failure edge case which would be difficult to test
without <code>Test::MockObject</code> because it requires control of the Perl
libraries installed. Testing this without <code>Test::MockObject</code> would
require altering the <code>@INC</code> in subtle ways or mocking a
<code>Geo::IP</code> package of your own. <code>Test::MockObject</code> does
that for you, so why bother to re-invent a wheel if you don't need to?</p></li>

<li><p>User has <code>Geo::IP</code> installed but does not have the city
data.</p>

<pre><code>use Test::More tests =&gt; 3;
use Test::Exception;
use Test::MockObject;

my $mock = Test::MockObject-&gt;new();
$mock-&gt;fake_module('Geo::IP' =&gt; (
    'open'           =&gt; sub { undef },
    'GEOIP_STANDARD' =&gt; sub { 0 }
));

use_ok('Site::Member');

my $u = Site::Member-&gt;new();
isa_ok($u, 'Site::Member');

$u-&gt;ip_address('64.40.146.219');

throws_ok {
    $u-&gt;city()
} qr/Could not create a Geo\:\:IP object/, '... got the error we expected';</code></pre>

<p>This next use case illustrates the use of <code>Test::MockObject</code> to
mock a dependency relationship, in particular the failure case where
<code>Geo::IP</code> cannot find the specified database file.</p>

<p><code>Geo::IP</code> follows the common Perl idiom of returning
<code>undef</code> if the object constructor fails. The example code tests for
this case and throws an exception if it comes up. Testing for this failure
uses the <code>fake_module</code> method again to hijack <code>Geo::IP</code>

and install a mocked version of its <code>open</code> method (the code also
fakes the <code>GEOIP_STANDARD</code> constant here). The mocked
<code>open</code> simply returns <code>undef</code> which will create the
proper conditions to trigger the exception in the example code.  The exception
is then caught using the <code>throws_ok</code> method of the <a
href="http://search.cpan.org/perldoc?Test::Exception">Test::Exception</a>

module.</p>

<p>This example illustrates that it is still possible to mock objects even if
your code is not in the position to pass in a mocked instance itself.  Again,
to test this without using <code>Test::MockObject</code> would require control
of the outside environment (the <code>Geo::IP database</code> file), or in some
way having control over where <code>Geo::IP</code> looks for the database file.
While well-written and well-architected code would probably allow you to alter
the database file path and therefore test this without using mock objects, the
mock object version makes no such assumptions and therefore works the same in
either case.</p></li>

<li><p>User has <code>Geo::IP</code> and the Geo-IP city data installed
correctly.</p>

<pre><code>use Test::More tests =&gt; 7;
use Test::MockObject;

my $mock = Test::MockObject-&gt;new();
$mock-&gt;fake_module('Geo::IP' =&gt; (
    'open'           =&gt; sub { $mock },
    'GEOIP_STANDARD' =&gt; sub { 0 }
));

my $mock_record = Test::MockObject-&gt;new();
$mock_record-&gt;set_always('city', 'New York City');

$mock-&gt;set_always('record_by_addr', $mock_record);

use_ok('Site::Member');

my $u = Site::Member-&gt;new();
isa_ok($u, 'Site::Member');

$u-&gt;ip_address('64.40.146.219');

is($u-&gt;city(), 'New York City', '... got the right city');

cmp_ok($mock-&gt;call_pos('record_by_addr'), '==', 0,
        '... our mock object was called');
is_deeply(
        [ $mock-&gt;call_args(0) ],
        [ $mock, '64.40.146.219' ],
        '... our mock was called with the right args');
        
cmp_ok($mock_record-&gt;call_pos('city'), '==', 0,
        '... our mock record object was called');
is_deeply(
        [ $mock_record-&gt;call_args(0) ],
        [ $mock_record ],
        '... our mock record was called with the right args');</code></pre>

<p>This next case illustrates a success case, where <code>Geo::IP</code> finds
the database file it wants and returns the expected results.</p>

<p>Once again, the <code>fake_module</code> method of
<code>Test::MockObject</code> mocks <code>Geo::IP</code>'s <code>open</code>

method, this time returning the <code>$mock</code> instance itself.  The code
creates another mock object, this time for the <code>Geo::IP::Record</code>
instance which <code>Geo::IP</code>'s <code>record_by_addr</code> returns.
<code>Test::MockObject</code>'s <code>set_always</code> method mocks the

<code>city</code> method for the <code>$mock_record</code> instance. After
this, <code>Geo::IP</code>'s <code>record_by_addr</code> is mocked to return
the <code>$mock_record</code> instance. With all of these mocks in place, the
tests then run. After that, inspecting the mock objects ensures that the code
called the correct methods on the mocked objects in the correct order and with
the correct arguments.</p>

<p>This example illustrates testing success without needing to worry about the
existence of an outside dependency. <code>Test::MockObject</code> supports
taking this test one step further and providing methods for inspecting the
details of the interaction between the example code and that of the mocked

<code>Geo::IP</code> module. Accomplishing this test without
<code>Test::MockObject</code> would be almost impossible given the lack of
control over the <code>Geo::IP</code> module and its internals.</p></li>

</ul>

<h3>Conclusion</h3>

<p>Mock objects can seem complex and overly abstract at first, but once grasped
they can be a simple, clean way to make hard things easy. I hope to have shown
how creating simple and minimal mock object with <code>Test::MockObject</code>
can help in testing cases which might be difficult using more traditional
means.</p>

        </div>



    </div>
    <div class="asset-footer"></div>
</div>




                            <div class="content-nav">
                                <a href="/pub/tools/">&laquo; Tools</a> |
                                <a href="/pub/">Main Index</a> |
                                <a href="/pub/archives.html">Archives</a>
                                | <a href="/pub/tutorials/">Tutorials &raquo;</a>
                            </div>


                        </div>
                    </div>


                    <div id="beta">
    <div id="beta-inner">


    
    <div class="widget-what-is-perl widget">
    <div class="widget-content widget-content-what-is-perl">
       Visit the home of the  Perl programming language: <a href="http://www.perl.org/">Perl.org</a
    </div>
</div>
<div class="widget-find-out-more widget-archives widget">
    <div class="widget-content">
        <ul>
            <li><a href="http://www.perl.org/get.html">Download</a></li>
            <li><a href="http://perldoc.perl.org/">Documentation</a></li>
            <li><a href="http://blogs.perl.org/">Perl Bloggers</a></li>
            <li><a href="http://news.perlfoundation.org/">Foundation News</a></li>
        </ul>
    </div>
</div><div class="widget-tcpc widget">
<h3 class="widget-header">Sponsored by</h3>
    <div class="widget-content">
        <a href="http://training.perl.com/" alt="Perl Training" target="_blank"><img src="/i/tcpc.png" width="150" height="50"></a>
    </div>
</div>

<div class="widget-syndication widget">
    <div class="widget-content">
        <ul>
            <li><img src="/mt-static/images/status_icons/feed.gif" alt="Subscribe to feed" width="9" height="9" /> <a href="/pub/atom.xml">Subscribe to this website's feed</a></li>

        </ul>
    </div>
</div>
<div class="widget-powered widget">
    <div class="widget-content">
        <a href="http://www.movabletype.com/"><img src="/mt-static/images/bug-pbmt-white.png" alt="Powered by Movable Type 5.02" width="120" height="75" /></a>
    </div>
</div>



    </div>
</div>






                </div>
            </div>


            <div id="footer">
    <div id="footer-inner">
        <div id="footer-content">
            <div class="widget-powered widget">
                <div class="widget-content">
                    Powered by <a href="http://www.movabletype.com/" rel="generator">Movable Type Pro</a>
                </div>
            </div>

        </div>
    </div>
</div>



        </div>
    </div>
</body>
</html>
