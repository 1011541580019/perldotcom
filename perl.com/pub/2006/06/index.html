<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" id="sixapart-standard">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="generator" content="Movable Type Pro 5.13-en" />
<link rel="stylesheet" href="/pub/styles.css" type="text/css" />
<link rel="start" href="/pub/" title="Home" />
<link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/pub/atom.xml" />
<script type="text/javascript" src="/pub/mt.js"></script>
<!--
<rdf:RDF xmlns="http://web.resource.org/cc/"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<Work rdf:about="/pub/">
<dc:title>Perl.com</dc:title>
<dc:description>news and views of the Perl programming language</dc:description>
<license rdf:resource="http://creativecommons.org/licenses/by-nc-nd/3.0/" />
</Work>
<License rdf:about="http://creativecommons.org/licenses/by-nc-nd/3.0/">
</License>
</rdf:RDF>
-->

<script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-50555-22']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type =
   'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
   'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0];
   s.parentNode.insertBefore(ga, s);
 })();

</script>
<script type='text/javascript' src='http://partner.googleadservices.com/gampad/google_service.js'></script>
<script type='text/javascript'>
GS_googleAddAdSenseService("ca-pub-4136420132070439");
GS_googleEnableAllServices();
</script>
<script type='text/javascript'>
GA_googleAddSlot("ca-pub-4136420132070439", "Perl_728x90");
</script>
<script type='text/javascript'>GA_googleFetchAds();</script>
    <title>Perl.com: June 2006 Archives</title>


    <link rel="prev" href="/pub/2006/05/" title="May 2006" />
    <link rel="next" href="/pub/2006/07/" title="July 2006" />

</head>
<body id="perl-com" class="mt-archive-listing mt-datebased-monthly-archive layout-wt">
    <div id="container">
        <div id="container-inner">


            <div id="header">
    <div id="header-inner">
        <div id="header-content">
        <span id="top_advert"> 
<!-- Put any landscape advert in here -->
<!-- Perl_728x90 -->
<script type='text/javascript'>
GA_googleFillSlot("Perl_728x90");
</script>
        </span> 



            <div id="header-name"><a href="/pub/" accesskey="1">Perl.com</a></div>
            <div id="header-description">news and views of the Perl programming language</div>




        </div>
    </div>
</div>



            <div id="content">
                <div id="content-inner">


                    <div id="alpha">
                        <div id="alpha-inner">


                            <h1 id="page-title" class="archive-title">June 2006 Archives</h1>





                            
                            <div id="entry-694" class="entry-asset asset hentry">
    <div class="asset-header">
        <h2 class="asset-name entry-title"><a href="/pub/2006/06/01/fear-api.html" rel="bookmark">FEAR-less Site Scraping</a></h2>
        <div class="asset-meta">
            <span class="byline">
    
                By <span class="vcard author">Yung-chung Lin</span> on <abbr class="published" title="2006-06-01T00:00:00-08:00">June  1, 2006 12:00 AM</abbr>
    
            </span>

            
            

        </div>
    </div>
    <div class="asset-content entry-content">

        <div class="asset-body">
            
<!-- sidebar begins -->
<!-- don't move sidebars-->
<!-- sidebar ends -->
<p>Imagine that you have an assignment that you need to fetch all of the web pages of a given website, scrape data from them, and transfer the data to another place, such as a database or plain files. This is a common scenario for data scraping tasks, and CPAN has plenty of modules for this job.</p>

<p>While I was developing site-scraping scripts, retrieving data from some sites of the same type, I realized that I had repeated many identical or very similar code structures, such as:</p>

<pre><code>  fetch_the_homepage();

  while(there_are_some_more_unfetched_links){
     foreach $link (@{links_in_the_current_page}){
         follow_link()          if $link =~ /NEXT_PAGE_OR_SOMETHING/;
         extract_product_spec() if $link =~ /PRODUCT_SPEC_PAGE/;
     }
  }</code></pre>

<h3>The Usual Tools</h3>

<p>At the very beginning, I created scripts using <code>LWP::Simple</code>, <code>LWP::UserAgent</code>, and vanilla regular expressions to extract links and produce details. As the number of scripts grew, I needed more powerful resources, so I started to use <code>WWW::Mechanize</code> for web page fetching and <code>Regexp::Bind</code>, <code>Template::Extract</code>, <code>HTML::LinkExtractor</code>, <code>Regexp::Common</code>, etc. for data scraping. However, then I still found many redundancies.</p>

<p>A scraping script first needs to use essential modules for the site scraping task. Second, it may need to instantiate objects. Third, site scraping involves many interactions among different modules, mostly by passing data between them. After you fetch a page, you may need to pass the page to <code>HTML::LinkExtractor</code> to extract links, to <code>Template::Extract</code> to get detailed information, or save it to a file. You may then store extracted data in a relational database. Considering these properties, creating a site scraping script is very time-consuming, and sometimes it makes a lot of duplication.</p>

<p>Thus, I tried to fuse some modules together, hoping to save some of my keystrokes and simplify the coding process.</p>

<h3>An Example using <code>WWW::Mechanize</code> and <code>Template::Extract</code></h3>

<p>Here's a typical site scraping script structure:</p>

<pre><code>     use YAML;
     use Data::Dumper;
     use WWW::Mechanize;
     use Template::Extract;

     my $mech = WWW::Mechanize-&gt;new();
     $mech-&gt;get( "http://search.cpan.org" );

     my $ext = Template::Extract-&gt;new;

     my @result = $ext-&gt;extract($template, $mech-&gt;content);
     print Dumper \@result;

     my @link;
     foreach ($mech-&gt;links){
         if( $_-&gt;[0] =~ /foo/ ) {
            $mech-&gt;get($_-&gt;[0]);
         }
         elsif( $_-&gt;[0] =~ /bar/ ) {
            push @link;
         }
         else {
            sub { 'do something here' }-&gt;($_-&gt;[0]);
         }
     }
     print $mech-&gt;content;
     print Dumper \@link;
     foreach (@result){
        print YAML::Dump $_;
     }</code></pre>

<p>This program does several things:</p>

<ul>
<li>Fetch CPAN's homepage.</li>

<li>Extract data with a template.</li>

<li>Process links using a control structure.</li>

<li>Print fetched content to <code>STDOUT</code>.</li>

<li>Dump links in the page.</li>

<li>Use YAML to print extract results.</li>
</ul>

<p>If you need to create just one or two temporary scripts, it is acceptable to use copy and paste to generate scripts. Things will become messy if the job is to create a hundred scripts and you still use copy and paste.</p>

<!-- sidebar begins -->
 <csperl file="grab" domain="on" record="b/990" template="b/article_sidebar2.view">
<!-- sidebar ends -->













<h3>What Do I Need?</h3>

<p>There are some techniques to gather identical code blocks and put them into some place and create scripts by loading different components for different purposes. Instead, I worked on the interface. I wished to simplify the problem through language. I re-examined the routine and the code structure to identified distinct features in every site scraping script:</p>

<ul>
<li>Manually load lots of modules.</li>

<li>Create a WWW agent.</li>

<li>Create an extractor object.</li>

<li>Process links using a control structure.</li>

<li>Perform extraction.</li>

<li>Process extracted results.</li>
</ul>

<p>I searched CPAN for something related to my ideas. I found plenty of modules for site scraping and data extraction, but no module that could meet my needs.</p>

<p>Then I created <code>FEAR::API</code>.</p>

<h3>Use FEAR::API</h3>

<p><code>FEAR::API</code>'s documentation says:</p>

<blockquote><code>FEAR::API</code> is a tool that helps reduce your time creating site scraping scripts and helps you do it in an much more elegant way. <code>FEAR::API</code> combines many strong and powerful features from various CPAN modules, such as <code>LWP::UserAgent</code>, <code>WWW::Mechanize</code>, <code>Template::Extract</code>, <code>Encode</code>, <code>HTML::Parser</code>, etc., and digests them into a deeper Zen.</blockquote>

<p>It might be best to introduce <code>FEAR::API</code> by rewriting the previous example:</p>

<pre><code>   1    use FEAR::API -base;
   2    url("search.cpan.org");
   3    fetch &gt;&gt; [
   4      qr(foo) =&gt; _feedback,
   5      qr(bar) =&gt; \my @link,
   6      qr()    =&gt; sub { 'do something here' }
   7    ];
   8    fetch while has_more_links;
   9    extmethod('Template::Extract');
  10    extract($template);
  11    print Dumper extresult;
  12    print document-&gt;as_string;
  13    print Dumper \@link;
  14    invoke_handler('YAML');</code></pre>

<p>Line 1 loads <code>FEAR::API</code>. The <code>-base</code> argument means the package is a subclass of <code>FEAR::API</code>. The module automatically instantiates <code>$_</code> as a <code>FEAR::API</code> object.</p>

<p>Line 2 specifies the URL. The code will later fetch this URL by calling <code>fetch()</code>, but you can use <code>fetch( $the_url )</code>, too.</p>

<p>Line 3 fetches the home page of <em>some.site.com</em>. <code>&gt;&gt;</code> is an overloaded operator for dispatching links. The following array reference contains pairs of (<code><em>regular expression</em> =&gt; <em>action</em></code>). An action can be a code ref, an array ref, or a <code>_feedback</code> or <code>_self</code> constant.</p>

<p><code>FEAR::API</code> maintains a queue of links. Using <code>_feedback</code> or <code>_self</code> means that <code>FEAR::API</code> should put the link in a queue for fetching later if the link matches a certain regular expression.</p>

<p>Line 8 calls <code>has_more_links</code>, so <code>FEAR::API</code> checks if the internal link queue has, well, more links. The program will continue fetching if there are queued links.</p>

<p>Line 9 specifies the extraction method. The default method is <code>Template::Extract</code>.</p>

<p>Line 10 extracts data according to <code>$template</code>.</p>

<p>Line 11 dumps the extracted results to <code>STDOUT</code>. <code>FEAR::API</code> even exports <code>Dumper()</code> for you. For <a href="http://search.cpan.org/perldoc?YAML">YAML</a> fans, there is also <code>Dump()</code>.</p>

<p>Line 12 accesses the fetched content through the object returned from <code>document</code>. You need to invoke <code>as_string()</code> to stringify the data. By the way, each fetched document is converted to UTF-8 automatically for you. It is very useful while processing multilingual texts.</p>

<p>Line 14 invokes the result handler to do data processing. The argument can be a subref, a module's name, <code>YAML</code>, or <code>Data::Dumper</code>.</p>

<h3>Comparison</h3>

<p>I hope that now you can see what <code>FEAR::API</code> has improved, at least in code size. <code>FEAR::API</code> encapsulates many modules, and you don't need to worry about messing around with them on your own. All you need to do is tell <code>FEAR::API</code> to fetch a page, to do extraction, and how you want to deal with links contained in the page and the extracted results from the page. You don't need to initialize a WWW agent, convert the encoding of a fetched page, create an extractor object on your own, pass content to the extractor, write control structures for link processing, or anything else. Everything happens inside of <code>FEAR::API</code> or via this simple syntax.</p>

<p>At first sight, perhaps you don't even realize that the example script uses OO. If you don't like things to happen so automatically, you may choose to drop the <code>-base</code> option. Then you have to create FEAR::API objects manually using <code>fear()</code>:</p>

<pre><code>   use FEAR::API;
   my $f = fear();</code></pre>

<p>One of the goals of <code>FEAR::API</code> is to weed out redundancies and minimize code size. It is very cumbersome to use syntax such as <code>$_-&gt;blah_blah('blah')</code> throughout a scraping script, given mass script creation requirements. I decided to remove <code>$_-&gt;</code>, while it still uses OO.</p>













<h3>More Features</h3>

<p><code>FEAR::API</code> incorporates many features from successful modules, and you can use <code>FEAR::API</code> as an alternative.</p>

<p>If you use <code>LWP::Simple</code>:</p>

<pre><code>   use LWP::Simple;
   get("http://search.cpan.org");
   getprint("http://search.cpan.org");
   getstore("http://search.cpan.org", 'cpan.html');</code></pre>

<p>With <code>FEAR::API</code>:</p>

<pre><code>   use FEAR::API;
   get("http://search.cpan.org");
   getprint("http://search.cpan.org");
   getstore("http://search.cpan.org", 'cpan.html');</code></pre>

<p>If you are familiar with <code>curl</code>, you may use:</p>

<pre><code>   $ <strong>curl  http://site.{one,two,three}.com</strong> 
   # and
   $ <strong>curl ftp://ftp.numericals.com/file[1-100].txt</strong></code></pre>

<p>In <code>FEAR::API</code>, use <code>Template-Toolkit</code>:</p>

<pre><code>   url("[% FOREACH number = ['one','two','three'] %]
        http://site.[% number %].com
        [% END %]");
   fetch while has_more_links;

   # and

   url("[% FOREACH number = [1..100] %]
        ftp://ftp.numericals.com/file[% number %].txt
        [% END %]");
   fetch while has_more_links;</code></pre>

<p><code>FEAR::API</code> also supports <code>WWW::Mechanize</code> methods. Use <code>submit_form()</code>, <code>links()</code>, and <code>follow_links()</code> in <code>FEAR::API</code> like those in <code>WWW::Mechanize</code>.</p>

<p>Submitting a query is easy:</p>

<pre><code>   fetch("http://search.cpan.org");
   submit_form(
               form_name =&gt; 'f',
               fields =&gt; {
                    query =&gt; 'perl'
               });
   template($template); # specify template
   extract;</code></pre>

<p>Dumping links is also easy:</p>

<pre><code>   print Dumper fetch("http://search.cpan.org/")-&gt;links;</code></pre>

<p>So is following links:</p>

<pre><code>   fetch("http://search.cpan.org/")-&gt;follow_link(n =&gt; 3);</code></pre>

<h4>Cleaning Up Content</h4>

<p>You may use <code>HTML::Strip</code> or basic regular expressions to strip HTML code in fetched content or in extracted results, but <code>FEAR::API</code> provides two simple methods: <code>preproc()</code> and <code>postproc()</code>. (There are also aliases: <code>doc_filter()</code> and <code>result_filter()</code>.)</p>

<p>You may process documents now with code resembling:</p>

<pre><code>   use LWP::Simple;
   use HTML::Strip;

   my $content = get("http://search.cpan.org");
   my $hs = HTML::Strip-&gt;new();
   print $hs-&gt;parse( $content );</code></pre>

<p>Things are easier in <code>FEAR::API</code>:</p>

<pre><code>   fetch("search.cpan.org");
   preproc(use =&gt; 'html_to_null');
   print document-&gt;as_string;</code></pre>

<p>If you don't use <code>FEAR::API</code> for postprocessing, your code might be:</p>

<pre><code>   use Data::Dumper;
   use LWP::Simple;
   use Template::Extract;
   my $extor = Template::Extract-&gt;new;
   my $content = get("http://search.cpan.org");
   my $result = $extor-&gt;extract($template, $content);
   foreach my $r (@$result){
      foreach (values %$r){
        s/(?:&lt;[^&gt;]*&gt;)+/ /g;
      }
   }
   print Dumper $result;</code></pre>

<p><code>FEAR::API</code> is simpler:</p>

<pre><code>   fetch("search.cpan.org");
   extract($template);
   postproc('s/(?:&lt;[^&gt;]*&gt;)+/ /g;');
   print extresult;</code></pre>

<p>You can apply <code>preproc()</code> and <code>postproc()</code> on data multiple times until you find satisfactory results.</p>

<h4>More Overloaded Operators.</h4>

<p>The previous examples have used the <code>dispatch_links</code> operator (<code>&gt;&gt;</code>). There are more overloaded operators that you can use to reduce your code size further.</p>

<pre><code>   print document-&gt;as_string;</code></pre>

<p>is equivalent to:</p>

<pre><code>   print $$_;</code></pre>

<pre><code>   print Dumper extresult;</code></pre>

<p>is equivalent to:</p>

<pre><code>   print Dumper \@$_;</code></pre>

<pre><code>   url("search.cpan.org")-&gt;();</code></pre>

<p>is equivalent to:</p>

<pre><code>   url("search.cpan.org");
   fetch;</code></pre>

<pre><code>   my $cont = fetch("search.cpan.org")-&gt;document-&gt;as_string;</code></pre>

<p>is equivalent to:</p>

<pre><code>   fetch("search.cpan.org") &gt; $cont;

   push my @cont, fetch("search.cpan.org")-&gt;document-&gt;as_string;</code></pre>

<p>is equivalent to:</p>

<pre><code>   fetch("search.cpan.org") &gt; \my @cont;</code></pre>

<h4>Filtering Syntax</h4>

<p><code>FEAR::API</code> creates something like shell piping. You can continually pass data through a series of filters to get what you need.</p>

<pre><code>   url("search.cpan.org")-&gt;()
     | _preproc(use =&gt; 'html_to_null')
     | _template($template)
     | _postproc('tr/a-z/A-Z/')
     | _foreach_result({ print Dumper $_ });</code></pre>

<p>This is equivalent to:</p>

<pre><code>   url("search.cpan.org")-&gt;();
   preproc(use =&gt; 'html_to_null');
   template($template);
   extract;
   postproc('tr/a-z/A-Z');
   foreach (@{extresult()}){
     print Dumper $_;
   }</code></pre>
   












<h3>A Full Example</h3>

<p>Finally, here is an example that submits a query to CPAN, extracts records of results, and then puts the extracted data into a <code>SQLite</code> database.</p>

<p>First, create the database schema:</p>

<pre><code>   % <strong>cat &gt; schema.sql</strong>

   CREATE TABLE cpan (
      module      varchar(64),
      dist        varchar(64),
      link        varchar(256),
      description varchar(128),
      date        varchar(32),
      author      varchar(64),
      url         varchar(256),
      primary key (module)
   );</code></pre>

<p>Then create the database:</p>

<pre><code>   % <strong>sqlite3 cpan.db &lt; schema.sql</strong></code></pre>

<p>Now create a class that maps to the database:</p>

<pre><code>   % <strong>mkdir lib</strong>
   % mkdir lib/CPAN
   % <strong>cat &gt; lib/CPAN/DBI.pm</strong>
   package CPAN::DBI;
   use base 'Class::DBI::SQLite';
   __PACKAGE__-&gt;set_db('Main', 'dbi:SQLite:dbname=cpan.db', '', '');
   __PACKAGE__-&gt;set_up_table('cpan');
   1;</code></pre>

<p>The next part is the CPAN scraper:</p>

<pre><code>  % <strong>cat &gt; cpan-scraper.pl
   use lib 'lib';
   use FEAR::API -base;
   use CPAN::DBI;

   url("http://search.cpan.org/")-&gt;();
   submit_form(form_name =&gt; 'f',
               fields =&gt; {
                  query =&gt; 'perl',
                  mode =&gt; 'module',
               });

   preproc('s/\A.+&lt;!--results--&gt;(.+)&lt;!--end results--&gt;.+\Z/$1/s');

   template('&lt;!--item--&gt;
  &lt;p&gt;&lt;a href="[% link %]"&gt;&lt;b&gt;[% module %]&lt;/b&gt;&lt;/a&gt;
&lt;br /&gt;&lt;small&gt;[% description %]&lt;/small&gt;
&lt;br /&gt;&lt;small&gt;   &lt;a href="[% ... %]"&gt;[% dist %]&lt;/a&gt; -
   &lt;span class=date&gt;[% date %]&lt;/span&gt; -
   &lt;a href="/~[% ... %]"&gt;[% author %]&lt;/a&gt;
&lt;/small&gt;
&lt;!--end item--&gt;');

   extract;

   invoke_handler(sub {
    print "-- Inserting $_-&gt;{module}\n";
    CPAN::DBI-&gt;find_or_create($_);
   });</strong></code></pre>

<p>Run it, and then check the database:</p>


<pre><code>   % <strong>sqlite3 cpan.db</strong>
   sqlite&gt; <strong>.mode csv</strong>
   sqlite&gt; <strong>select module, dist, author from cpan;</strong></code></pre>

<p>If everything goes well, your results will resemble:</p>

<pre><code>   "Perl","PerlInterp-0.03","Ben Morrow"
   "Perl::AfterFork","Perl-AfterFork-0.01","Torsten F&amp;#246;rtsch"
   "Perl::AtEndOfScope","Perl-AtEndOfScope-0.01","Torsten F&amp;#246;rtsch"
   "Perl::BestPractice","Perl-BestPractice-0.01","Adam Kennedy"
   "Perl::Compare","Perl-Compare-0.10","Adam Kennedy"
   "Perl::Critic","Perl-Critic-0.14","Jeffrey Ryan Thalhammer"
   "Perl::Editor","Perl-Editor-0.02","Adam Kennedy"
   "Perl::Metrics","Perl-Metrics-0.05","Adam Kennedy"
   "Perl::MinimumVersion","Perl-MinimumVersion-0.11","Adam Kennedy"
   "Perl::SAX","Perl-SAX-0.06","Adam Kennedy"</code></pre>

<p>Isn't that easy?</p>

<h3>Conclusion</h3>

<p><code>FEAR::API</code> is an innovation for site scraping. It combines strong features and powerful methods from various modules, and it also employs operator overloading to build something a domain-specific language without forbidding the use of Perl's full power. <code>FEAR::API</code> is very suitable for the fast creation of scraping scripts. A central dogma of <code>FEAR::API</code> is "Code the least and perform the most."</p>

<p>However, <code>FEAR::API</code> still needs lots of improvement. Currently, it does not handle errors very well, lacks automatic template generation, performs no logging, and has no direct connection to a database mapper such as <code>DBIx::Class</code> or <code>Class::DBI</code>. Even the documentation needs work.</p>

<p>Patches or suggestions are welcome!</p>
        </div>



    </div>
    <div class="asset-footer"></div>
</div>




                            <div class="content-nav">
                                <a href="/pub/2006/05/">&laquo; May 2006</a> |
                                <a href="/pub/">Main Index</a> |
                                <a href="/pub/archives.html">Archives</a>
                                | <a href="/pub/2006/07/">July 2006 &raquo;</a>
                            </div>


                        </div>
                    </div>


                    <div id="beta">
    <div id="beta-inner">


    
    <div class="widget-what-is-perl widget">
    <div class="widget-content widget-content-what-is-perl">
       Visit the home of the  Perl programming language: <a href="http://www.perl.org/">Perl.org</a
    </div>
</div>
<div class="widget-find-out-more widget-archives widget">
    <div class="widget-content">
        <ul>
            <li><a href="http://www.perl.org/get.html">Download</a></li>
            <li><a href="http://perldoc.perl.org/">Documentation</a></li>
            <li><a href="http://blogs.perl.org/">Perl Bloggers</a></li>
            <li><a href="http://news.perlfoundation.org/">Foundation News</a></li>
        </ul>
    </div>
</div><div class="widget-tcpc widget">
<h3 class="widget-header">Sponsored by</h3>
    <div class="widget-content">
        <a href="http://training.perl.com/" alt="Perl Training" target="_blank"><img src="/i/tcpc.png" width="150" height="50"></a>
    </div>
</div>

<div class="widget-syndication widget">
    <div class="widget-content">
        <ul>
            <li><img src="/mt-static/images/status_icons/feed.gif" alt="Subscribe to feed" width="9" height="9" /> <a href="/pub/atom.xml">Subscribe to this website's feed</a></li>

        </ul>
    </div>
</div>
<div class="widget-archive-monthly widget-archive widget">
    <h3 class="widget-header">Monthly <a href="/pub/archives.html">Archives</a></h3>
    <div class="widget-content">
        <ul>
        
            <li><a href="/pub/2014/02/">February 2014 (1)</a></li>
        
    
        
            <li><a href="/pub/2014/01/">January 2014 (1)</a></li>
        
    
        
            <li><a href="/pub/2013/10/">October 2013 (1)</a></li>
        
    
        
            <li><a href="/pub/2013/01/">January 2013 (1)</a></li>
        
    
        
            <li><a href="/pub/2012/12/">December 2012 (1)</a></li>
        
    
        
            <li><a href="/pub/2012/11/">November 2012 (1)</a></li>
        
    
        
            <li><a href="/pub/2012/10/">October 2012 (2)</a></li>
        
    
        
            <li><a href="/pub/2012/08/">August 2012 (2)</a></li>
        
    
        
            <li><a href="/pub/2012/06/">June 2012 (11)</a></li>
        
    
        
            <li><a href="/pub/2012/05/">May 2012 (18)</a></li>
        
    
        
            <li><a href="/pub/2012/04/">April 2012 (17)</a></li>
        
    
        
            <li><a href="/pub/2012/02/">February 2012 (1)</a></li>
        
    
        
            <li><a href="/pub/2011/12/">December 2011 (1)</a></li>
        
    
        
            <li><a href="/pub/2011/09/">September 2011 (1)</a></li>
        
    
        
            <li><a href="/pub/2011/08/">August 2011 (2)</a></li>
        
    
        
            <li><a href="/pub/2011/06/">June 2011 (1)</a></li>
        
    
        
            <li><a href="/pub/2011/05/">May 2011 (3)</a></li>
        
    
        
            <li><a href="/pub/2011/04/">April 2011 (1)</a></li>
        
    
        
            <li><a href="/pub/2011/03/">March 2011 (1)</a></li>
        
    
        
            <li><a href="/pub/2011/02/">February 2011 (1)</a></li>
        
    
        
            <li><a href="/pub/2011/01/">January 2011 (1)</a></li>
        
    
        
            <li><a href="/pub/2010/11/">November 2010 (1)</a></li>
        
    
        
            <li><a href="/pub/2010/10/">October 2010 (2)</a></li>
        
    
        
            <li><a href="/pub/2010/09/">September 2010 (1)</a></li>
        
    
        
            <li><a href="/pub/2010/08/">August 2010 (3)</a></li>
        
    
        
            <li><a href="/pub/2010/07/">July 2010 (2)</a></li>
        
    
        
            <li><a href="/pub/2010/04/">April 2010 (2)</a></li>
        
    
        
            <li><a href="/pub/2010/03/">March 2010 (4)</a></li>
        
    
        
            <li><a href="/pub/2008/05/">May 2008 (1)</a></li>
        
    
        
            <li><a href="/pub/2008/04/">April 2008 (2)</a></li>
        
    
        
            <li><a href="/pub/2008/03/">March 2008 (1)</a></li>
        
    
        
            <li><a href="/pub/2008/02/">February 2008 (1)</a></li>
        
    
        
            <li><a href="/pub/2008/01/">January 2008 (1)</a></li>
        
    
        
            <li><a href="/pub/2007/12/">December 2007 (2)</a></li>
        
    
        
            <li><a href="/pub/2007/09/">September 2007 (1)</a></li>
        
    
        
            <li><a href="/pub/2007/08/">August 2007 (1)</a></li>
        
    
        
            <li><a href="/pub/2007/07/">July 2007 (1)</a></li>
        
    
        
            <li><a href="/pub/2007/06/">June 2007 (1)</a></li>
        
    
        
            <li><a href="/pub/2007/05/">May 2007 (1)</a></li>
        
    
        
            <li><a href="/pub/2007/04/">April 2007 (1)</a></li>
        
    
        
            <li><a href="/pub/2007/03/">March 2007 (1)</a></li>
        
    
        
            <li><a href="/pub/2007/02/">February 2007 (1)</a></li>
        
    
        
            <li><a href="/pub/2007/01/">January 2007 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/12/">December 2006 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/11/">November 2006 (2)</a></li>
        
    
        
            <li><a href="/pub/2006/10/">October 2006 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/09/">September 2006 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/08/">August 2006 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/07/">July 2006 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/06/">June 2006 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/05/">May 2006 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/04/">April 2006 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/03/">March 2006 (1)</a></li>
        
    
        
            <li><a href="/pub/2006/02/">February 2006 (4)</a></li>
        
    
        
            <li><a href="/pub/2006/01/">January 2006 (4)</a></li>
        
    
        
            <li><a href="/pub/2005/12/">December 2005 (4)</a></li>
        
    
        
            <li><a href="/pub/2005/11/">November 2005 (3)</a></li>
        
    
        
            <li><a href="/pub/2005/10/">October 2005 (2)</a></li>
        
    
        
            <li><a href="/pub/2005/09/">September 2005 (2)</a></li>
        
    
        
            <li><a href="/pub/2005/08/">August 2005 (9)</a></li>
        
    
        
            <li><a href="/pub/2005/07/">July 2005 (8)</a></li>
        
    
        
            <li><a href="/pub/2005/06/">June 2005 (9)</a></li>
        
    
        
            <li><a href="/pub/2005/05/">May 2005 (8)</a></li>
        
    
        
            <li><a href="/pub/2005/04/">April 2005 (7)</a></li>
        
    
        
            <li><a href="/pub/2005/03/">March 2005 (6)</a></li>
        
    
        
            <li><a href="/pub/2005/02/">February 2005 (7)</a></li>
        
    
        
            <li><a href="/pub/2005/01/">January 2005 (6)</a></li>
        
    
        
            <li><a href="/pub/2004/12/">December 2004 (8)</a></li>
        
    
        
            <li><a href="/pub/2004/11/">November 2004 (8)</a></li>
        
    
        
            <li><a href="/pub/2004/10/">October 2004 (5)</a></li>
        
    
        
            <li><a href="/pub/2004/09/">September 2004 (9)</a></li>
        
    
        
            <li><a href="/pub/2004/08/">August 2004 (6)</a></li>
        
    
        
            <li><a href="/pub/2004/07/">July 2004 (8)</a></li>
        
    
        
            <li><a href="/pub/2004/06/">June 2004 (6)</a></li>
        
    
        
            <li><a href="/pub/2004/05/">May 2004 (7)</a></li>
        
    
        
            <li><a href="/pub/2004/04/">April 2004 (8)</a></li>
        
    
        
            <li><a href="/pub/2004/03/">March 2004 (8)</a></li>
        
    
        
            <li><a href="/pub/2004/02/">February 2004 (9)</a></li>
        
    
        
            <li><a href="/pub/2004/01/">January 2004 (7)</a></li>
        
    
        
            <li><a href="/pub/2003/12/">December 2003 (4)</a></li>
        
    
        
            <li><a href="/pub/2003/11/">November 2003 (7)</a></li>
        
    
        
            <li><a href="/pub/2003/10/">October 2003 (8)</a></li>
        
    
        
            <li><a href="/pub/2003/09/">September 2003 (6)</a></li>
        
    
        
            <li><a href="/pub/2003/08/">August 2003 (7)</a></li>
        
    
        
            <li><a href="/pub/2003/07/">July 2003 (9)</a></li>
        
    
        
            <li><a href="/pub/2003/06/">June 2003 (9)</a></li>
        
    
        
            <li><a href="/pub/2003/05/">May 2003 (8)</a></li>
        
    
        
            <li><a href="/pub/2003/04/">April 2003 (8)</a></li>
        
    
        
            <li><a href="/pub/2003/03/">March 2003 (10)</a></li>
        
    
        
            <li><a href="/pub/2003/02/">February 2003 (8)</a></li>
        
    
        
            <li><a href="/pub/2003/01/">January 2003 (8)</a></li>
        
    
        
            <li><a href="/pub/2002/12/">December 2002 (5)</a></li>
        
    
        
            <li><a href="/pub/2002/11/">November 2002 (9)</a></li>
        
    
        
            <li><a href="/pub/2002/10/">October 2002 (7)</a></li>
        
    
        
            <li><a href="/pub/2002/09/">September 2002 (11)</a></li>
        
    
        
            <li><a href="/pub/2002/08/">August 2002 (8)</a></li>
        
    
        
            <li><a href="/pub/2002/07/">July 2002 (8)</a></li>
        
    
        
            <li><a href="/pub/2002/06/">June 2002 (4)</a></li>
        
    
        
            <li><a href="/pub/2002/05/">May 2002 (6)</a></li>
        
    
        
            <li><a href="/pub/2002/04/">April 2002 (6)</a></li>
        
    
        
            <li><a href="/pub/2002/03/">March 2002 (7)</a></li>
        
    
        
            <li><a href="/pub/2002/02/">February 2002 (5)</a></li>
        
    
        
            <li><a href="/pub/2002/01/">January 2002 (8)</a></li>
        
    
        
            <li><a href="/pub/2001/12/">December 2001 (7)</a></li>
        
    
        
            <li><a href="/pub/2001/11/">November 2001 (5)</a></li>
        
    
        
            <li><a href="/pub/2001/10/">October 2001 (9)</a></li>
        
    
        
            <li><a href="/pub/2001/09/">September 2001 (7)</a></li>
        
    
        
            <li><a href="/pub/2001/08/">August 2001 (13)</a></li>
        
    
        
            <li><a href="/pub/2001/07/">July 2001 (8)</a></li>
        
    
        
            <li><a href="/pub/2001/06/">June 2001 (13)</a></li>
        
    
        
            <li><a href="/pub/2001/05/">May 2001 (11)</a></li>
        
    
        
            <li><a href="/pub/2001/04/">April 2001 (9)</a></li>
        
    
        
            <li><a href="/pub/2001/03/">March 2001 (8)</a></li>
        
    
        
            <li><a href="/pub/2001/02/">February 2001 (8)</a></li>
        
    
        
            <li><a href="/pub/2001/01/">January 2001 (8)</a></li>
        
    
        
            <li><a href="/pub/2000/12/">December 2000 (6)</a></li>
        
    
        
            <li><a href="/pub/2000/11/">November 2000 (10)</a></li>
        
    
        
            <li><a href="/pub/2000/10/">October 2000 (10)</a></li>
        
    
        
            <li><a href="/pub/2000/09/">September 2000 (2)</a></li>
        
    
        
            <li><a href="/pub/2000/08/">August 2000 (2)</a></li>
        
    
        
            <li><a href="/pub/2000/07/">July 2000 (5)</a></li>
        
    
        
            <li><a href="/pub/2000/06/">June 2000 (7)</a></li>
        
    
        
            <li><a href="/pub/2000/05/">May 2000 (7)</a></li>
        
    
        
            <li><a href="/pub/2000/04/">April 2000 (3)</a></li>
        
    
        
            <li><a href="/pub/2000/03/">March 2000 (2)</a></li>
        
    
        
            <li><a href="/pub/2000/02/">February 2000 (2)</a></li>
        
    
        
            <li><a href="/pub/2000/01/">January 2000 (2)</a></li>
        
    
        
            <li><a href="/pub/1999/12/">December 1999 (6)</a></li>
        
    
        
            <li><a href="/pub/1999/11/">November 1999 (6)</a></li>
        
    
        
            <li><a href="/pub/1999/10/">October 1999 (5)</a></li>
        
    
        
            <li><a href="/pub/1999/09/">September 1999 (4)</a></li>
        
    
        
            <li><a href="/pub/1999/08/">August 1999 (3)</a></li>
        
    
        
            <li><a href="/pub/1999/07/">July 1999 (2)</a></li>
        
    
        
            <li><a href="/pub/1999/06/">June 1999 (3)</a></li>
        
    
        
            <li><a href="/pub/1999/04/">April 1999 (1)</a></li>
        
    
        
            <li><a href="/pub/1999/03/">March 1999 (1)</a></li>
        
    
        
            <li><a href="/pub/1999/01/">January 1999 (1)</a></li>
        
    
        
            <li><a href="/pub/1998/12/">December 1998 (1)</a></li>
        
    
        
            <li><a href="/pub/1998/11/">November 1998 (1)</a></li>
        
    
        
            <li><a href="/pub/1998/07/">July 1998 (2)</a></li>
        
    
        
            <li><a href="/pub/1998/06/">June 1998 (1)</a></li>
        
    
        
            <li><a href="/pub/1998/03/">March 1998 (1)</a></li>
        
        </ul>
    </div>
</div>
        
    

<div class="widget-powered widget">
    <div class="widget-content">
        <a href="http://www.movabletype.com/"><img src="/mt-static/images/bug-pbmt-white.png" alt="Powered by Movable Type 5.13-en" width="120" height="75" /></a>
    </div>
</div>



    </div>
</div>






                </div>
            </div>


            <div id="footer">
    <div id="footer-inner">
        <div id="footer-content">
            <div class="widget-powered widget">
                <div class="widget-content">
                    Powered by <a href="http://www.movabletype.com/" rel="generator">Movable Type Pro</a>
                </div>
            </div>

            <div class="widget-creative-commons widget">
                <div class="widget-content">
                    This blog is licensed under a <a href="http://creativecommons.org/licenses/by-nc-nd/3.0/">Creative Commons License</a>.
                </div>
            </div>

        </div>
    </div>
</div>



        </div>
    </div>
</body>
</html>
